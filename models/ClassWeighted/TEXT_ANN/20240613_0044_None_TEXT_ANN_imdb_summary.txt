20240613_0044_None_TEXT_ANN_imdb_summaryHyperparametersexp_id:	20240613_0044
data_name:	imdb
model_name:	text_ann
epochs:		300
learning_rate:	0.001

X_train.shape:	720
X_val.shape:	80
X_test.shape:	200
y_train.shape:	(720,)

y_val.shape:	(80,)

y_test.shape:	(200,)

dim_per_layer:	[None]
activation_per_layer:	[None]

F1-score: 0.7324 | Accuracy: 0.7625 | AUC-ROC: 0.8139

Proportion of ones in val set: 0.512
Proportion of ones predicted in test set: 0.367
Proportion of ones in train set: 0.479
Proportion of ones predicted in train set: 0.478

Architecture:
OrderedDict([('embeddings', TokenEmbedding(
  (embedding): Embedding(2595, 8)
)), ('pos_encoder', PositionalEncoding(
  (dropout): Dropout(p=0.3, inplace=False)
)), ('encoder', TransformerEncoder(
  (layers): ModuleList(
    (0): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
      )
      (linear1): Linear(in_features=8, out_features=32, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (linear2): Linear(in_features=32, out_features=8, bias=True)
      (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0.1, inplace=False)
      (dropout2): Dropout(p=0.1, inplace=False)
    )
  )
)), ('linear', Linear(in_features=8, out_features=2, bias=True)), ('dropout', Dropout(p=0.3, inplace=False))])