{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# go up a directory\n",
    "import os\n",
    "os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T03:47:28.503235Z",
     "start_time": "2024-06-14T03:47:28.501137Z"
    }
   },
   "id": "608e8f43b9f3ee0e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get cwd\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae88dca11cc366bd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from openxai.dataloader import return_loaders, get_tokenizer_and_vocab\n",
    "import numpy as np\n",
    "from openxai.ML_Models.LR.model import LogisticRegression\n",
    "import openxai.ML_Models.ANN.MLP as model_MLP\n",
    "import openxai.ML_Models.ANN.Text_MLP as model_MLP_Text\n",
    "import datetime\n",
    "from sklearn.metrics import auc\n",
    "from utils import get_model_names"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:47:45.159Z",
     "start_time": "2024-06-14T18:47:45.152239Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1545
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculateFaithfulnessAUC_text(test_sentences, explanations, text_classifier, min_idx, max_idx, max_k, do_pgu=False, do_random_baseline=False):\n",
    "    PG_AUC = []\n",
    "    # sentence_pointer = 0\n",
    "    # explanation_pointer = 0\n",
    "    # skipped = False\n",
    "    for index, _ in enumerate(test_sentences):\n",
    "        # if skipped:\n",
    "        #     skipped = False\n",
    "        #     sentence_pointer += 1\n",
    "        #     continue\n",
    "        test_sentence = test_sentences[index]\n",
    "        # print('test_sentence', test_sentence)\n",
    "        # # print('sentence_pointer', sentence_pointer)\n",
    "        if index in bad_reply_indices + manual_skip_sentence_index:\n",
    "#             print('-------------------\\n\\n\\n')\n",
    "#             print(f'Skipping {index} because it has a bad reply from the LLM, {all_exps[index]}')\n",
    "            # sentence_pointer += 1\n",
    "            # explanation_pointer += 1\n",
    "            continue\n",
    "\n",
    "#         # print('len(tokenizer(test_sentence))',len(tokenizer(test_sentence)))\n",
    "        tokenized_sentence = tokenizer(test_sentence)\n",
    "        num_tokens = len(tokenized_sentence)\n",
    "#         print('num_tokens', num_tokens)\n",
    "        if num_tokens < min_num_tokens_needed:\n",
    "#             print(f'Skipping {index} because it has too few tokens ({num_tokens})')\n",
    "#             print(f'Need at least {min_num_tokens_needed} tokens')\n",
    "            # sentence_pointer += 1\n",
    "            \n",
    "            # skipped = True\n",
    "            continue\n",
    "        if index == max_idx:\n",
    "            break\n",
    "        if max_k > 1:\n",
    "            auc_x = np.arange(max_k) / (max_k - 1)\n",
    "        PG = []\n",
    "        for top_k in range(1, max_k + 1):\n",
    "#             print('explanation:', explanations[index][0][:top_k])\n",
    "            PG.append(\n",
    "                PG_words(test_sentence, explanations[index][0][:top_k], text_classifier, do_pgu, do_random_baseline)) \n",
    "        if max_k > 1:\n",
    "            PG_AUC.append(auc(auc_x, PG))\n",
    "        else:\n",
    "            PG_AUC.append(PG)\n",
    "        # explanation_pointer += 1\n",
    "        # sentence_pointer += 1\n",
    "    return PG_AUC\n",
    "\n",
    "def PG_words(test_sentence, topk_exp_words, text_classifier, do_pgu, do_random_baseline=False):\n",
    "#     print('test_sentence', test_sentence)\n",
    "#     print('IN PG WORDS FUNCTION')\n",
    "    tokenized_input = torch.tensor([voc[t] for t in tokenizer(test_sentence)])\n",
    "    with torch.no_grad():\n",
    "        pred_original = text_classifier(tokenized_input.unsqueeze(0))\n",
    "\n",
    "    if do_random_baseline:\n",
    "        # Create a mask where each element is True if it is NOT in values_to_remove\n",
    "        mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "        if not do_pgu:\n",
    "            while mask.sum() == 0:\n",
    "                mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "        else:\n",
    "            while mask.sum() == len(tokenized_input):\n",
    "                mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "\n",
    "    else:\n",
    "        # find the indices where the topk words are in the sentence\n",
    "#         # print('topk_exp_words', topk_exp_words)\n",
    "        voc_values_to_remove = []\n",
    "#         print('topk_exp_words', topk_exp_words)\n",
    "        for word in topk_exp_words:\n",
    "#             print('word', word)\n",
    "            top_k_idx_to_remove = voc[tokenizer(word)[0]]\n",
    "            voc_values_to_remove.append(top_k_idx_to_remove)\n",
    "        \n",
    "        # Create a mask where each element is True if it is NOT in values_to_remove\n",
    "#         print('tokenized_input', tokenized_input)\n",
    "#         print('voc_values_to_remove', voc_values_to_remove)\n",
    "        mask = ~torch.isin(tokenized_input, torch.tensor(voc_values_to_remove))\n",
    "    \n",
    "    if do_pgu:\n",
    "#         print('flipping mask!')\n",
    "#         print('mask', mask)\n",
    "        # flip the mask\n",
    "        mask = ~mask\n",
    "#         print('flipped mask', mask)\n",
    "        \n",
    "    # Apply the mask to get the filtered tensor\n",
    "    filtered_tokenized_input = tokenized_input[mask]\n",
    "#     print('tokenized_input', tokenized_input)\n",
    "#     print('mask', mask)\n",
    "#     print('filtered_tokenized_input', filtered_tokenized_input)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_removed = text_classifier(filtered_tokenized_input.unsqueeze(0))\n",
    "        \n",
    "#     print('pred_original', pred_original)\n",
    "#     print('pred_removed', pred_removed)\n",
    "    PG = torch.abs(pred_original.squeeze() - pred_removed.squeeze())[0] # [0] - take first class. assume binary classifier (this is what openxai does) \n",
    "#     print(\"FINISHED PG WORDS FUNCTION\")\n",
    "    return PG"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:47:45.489124Z",
     "start_time": "2024-06-14T18:47:45.481825Z"
    }
   },
   "id": "d8c9e4081edd1917",
   "execution_count": 1546
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def classifier_fn(perturbed_texts):\n",
    "    all_tokenized_sentences = []\n",
    "    for text in perturbed_texts:\n",
    "        temp_texts = []\n",
    "        for t in tokenizer(text):\n",
    "            temp_texts.append(voc[t])\n",
    "        all_tokenized_sentences.append(temp_texts)\n",
    "    \n",
    "    max_len = max([len(tokens) for tokens in all_tokenized_sentences])\n",
    "    for sentence in all_tokenized_sentences:\n",
    "        while len(sentence) < max_len:\n",
    "            sentence.append(0) # pad token\n",
    "    all_tokenized_sentences = [torch.tensor(sentence) for sentence in all_tokenized_sentences]\n",
    "    inputs = torch.stack(all_tokenized_sentences)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:47:46.234290Z",
     "start_time": "2024-06-14T18:47:46.231046Z"
    }
   },
   "id": "214a42fba8659b48",
   "execution_count": 1547
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def getExperimentID():\n",
    "    date_info = datetime.datetime.now()\n",
    "    testID    = '%d%02d%02d_%02d%02d' % (date_info.year, date_info.month, date_info.day, date_info.hour, date_info.minute)\n",
    "    return testID"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:47:46.520400Z",
     "start_time": "2024-06-14T18:47:46.518285Z"
    }
   },
   "id": "5f302bb0bd8de99f",
   "execution_count": 1548
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def DefineModel(model_name, dim_per_layer=None, activation_per_layer=None, vocab_size=None, embed_dim=None, num_class=None):\n",
    "    if 'text_ann' in model_name:\n",
    "        # model = model_MLP_Text.Text_MLP(vocab_size, embed_dim, num_class)\n",
    "        model = model_MLP_Text.Text_MLP(vocab_size)\n",
    "    else:\n",
    "        input_size = loader_train.dataset.get_number_of_features()\n",
    "        if 'ann' in model_name:\n",
    "            dim_per_layer = [input_size] + dim_per_layer\n",
    "            model         = model_MLP.MLP(dim_per_layer, activation_per_layer)\n",
    "        elif model_name == 'lr':\n",
    "            dim_per_layer = [input_size] + dim_per_layer\n",
    "            model         = LogisticRegression(dim_per_layer[0], dim_per_layer[1])\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:47:46.927890Z",
     "start_time": "2024-06-14T18:47:46.925237Z"
    }
   },
   "id": "7c5a5196a5b48f1b",
   "execution_count": 1549
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 gpt-4-0125-preview amazon_1000 v1\n",
      "Vocabulary size: 1580\n",
      "Vocabulary size: 1580\n",
      "PGI:0.39+/-0.041\n",
      "PGU:0.252+/-0.037\n",
      "16 gpt-4-0125-preview yelp v1\n",
      "Vocabulary size: 1721\n",
      "Vocabulary size: 1721\n",
      "PGI:0.3+/-0.038\n",
      "PGU:0.231+/-0.033\n",
      "16 gpt-4-0125-preview imdb v1\n",
      "Vocabulary size: 2595\n",
      "Vocabulary size: 2595\n",
      "PGI:0.295+/-0.04\n",
      "PGU:0.263+/-0.039\n",
      "16 gpt-4 yelp v1\n",
      "Vocabulary size: 1721\n",
      "Vocabulary size: 1721\n",
      "PGI:0.3+/-0.036\n",
      "PGU:0.293+/-0.037\n",
      "16 gpt-4 yelp pgicl\n",
      "Vocabulary size: 1721\n",
      "Vocabulary size: 1721\n",
      "PGI:0.349+/-0.039\n",
      "PGU:0.243+/-0.033\n",
      "16 gpt-4 amazon_1000 v1\n",
      "Vocabulary size: 1580\n",
      "Vocabulary size: 1580\n",
      "PGI:0.409+/-0.038\n",
      "PGU:0.222+/-0.036\n",
      "16 gpt-4 amazon_1000 pgicl\n",
      "Vocabulary size: 1580\n",
      "Vocabulary size: 1580\n",
      "PGI:0.386+/-0.038\n",
      "PGU:0.238+/-0.036\n",
      "16 gpt-4 imdb v1\n",
      "Vocabulary size: 2595\n",
      "Vocabulary size: 2595\n",
      "PGI:0.332+/-0.038\n",
      "PGU:0.205+/-0.031\n",
      "16 gpt-4 imdb pgicl\n",
      "Vocabulary size: 2595\n",
      "Vocabulary size: 2595\n",
      "PGI:0.318+/-0.043\n",
      "PGU:0.251+/-0.038\n",
      "16 gpt-4-0125-preview yelp pgicl\n",
      "Vocabulary size: 1721\n",
      "Vocabulary size: 1721\n",
      "PGI:0.244+/-0.035\n",
      "PGU:0.26+/-0.034\n",
      "16 gpt-4-0125-preview amazon_1000 pgicl\n",
      "Vocabulary size: 1580\n",
      "Vocabulary size: 1580\n",
      "PGI:0.371+/-0.042\n",
      "PGU:0.205+/-0.035\n",
      "16 gpt-4-0125-preview imdb pgicl\n",
      "Vocabulary size: 2595\n",
      "Vocabulary size: 2595\n",
      "PGI:0.329+/-0.045\n",
      "PGU:0.214+/-0.039\n"
     ]
    }
   ],
   "source": [
    "files_dirs = [\n",
    "    '20240612_202637_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_v1_amazon_1000_text_ann',\n",
    "    '20240613_014033_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_v1_yelp_text_ann',\n",
    "    '20240613_022348_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_v1_imdb_text_ann',\n",
    "    '20240613_143413_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_v1_yelp_text_ann',\n",
    "    '20240613_144431_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_pgicl_yelp_text_ann',\n",
    "    '20240613_151903_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_v1_amazon_1000_text_ann',\n",
    "    '20240613_152935_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_pgicl_amazon_1000_text_ann',\n",
    "    '20240613_160409_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_v1_imdb_text_ann',\n",
    "    '20240613_161500_gpt-4_nshot16_k3_prompt-senti_classif_remove_words_pgicl_imdb_text_ann',\n",
    "    '20240613_232429_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_pgicl_yelp_text_ann',\n",
    "    '20240614_000552_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_pgicl_amazon_1000_text_ann',\n",
    "    '20240614_004547_gpt-4-0125-preview_nshot16_k3_prompt-senti_classif_remove_words_pgicl_imdb_text_ann'\n",
    " ]\n",
    "\n",
    "for files_dir in files_dirs:\n",
    "    files_dir = f'outputs/LLM_QueryAndReply/{files_dir}/'\n",
    "    \n",
    "    # list files in directory that end in summary.txt\n",
    "    txt_file_replies = glob.glob(files_dir + '*summary.txt')\n",
    "    n_shot = int(files_dir.split('_')[4].split('shot')[1])\n",
    "    min_num_tokens_needed = int(np.log2(n_shot))\n",
    "    # get the gpt model version\n",
    "    llm_name = files_dir.split('/')[-2].split('_')[2]\n",
    "    data_name = files_dir.split('/')[-2].split('_')[-3]\n",
    "    prompt_strategy = files_dir.split('/')[-2].split('_')[-4]\n",
    "    if data_name == '1000':\n",
    "        data_name = 'amazon_1000'\n",
    "        prompt_strategy = files_dir.split('/')[-2].split('_')[-5]\n",
    "    \n",
    "    print(n_shot, llm_name, data_name, prompt_strategy)\n",
    "    \n",
    "    # sort files so that they're read in numerical order: 0_gpt-4-0125-preview_ANN_L_beauty_summary.txt, 1_gpt-4-0125-preview_ANN_L_beauty_summary.txt, etc.\n",
    "    txt_file_replies = sorted(txt_file_replies, key=lambda x: int(x.split('/')[-1].split('_')[0]))\n",
    "    # txt_file_replies\n",
    "    # read in each file\n",
    "    replies = []\n",
    "    for file in txt_file_replies:\n",
    "        with open(file, 'r') as f:\n",
    "            replies.append(f.read())\n",
    "    # get the eval_idx from reply\n",
    "    # '...eval_idx:\\t\\t1...'\n",
    "    eval_inds = [int(reply.split('eval_idx:\\t\\t')[1].split('\\n')[0]) for reply in replies]\n",
    "    # strip and trailing \\n get the last line of the file\n",
    "    replies = [reply.strip().split('\\n')[-1].split(',') for reply in replies]\n",
    "    #strip each word of leading and trailing whitespace\n",
    "    replies = [[[word.strip() for word in reply]] for reply in replies]\n",
    "    # strip Answer: from '['Answer: the', 'of', 'was']'\n",
    "    for r, reply in enumerate(replies):\n",
    "        word = reply[0][0]\n",
    "        if 'Answer:' in word:\n",
    "            replies[r][0][0] = word.split('Answer: ')[1]\n",
    "        if 'Final rank:' in word:\n",
    "            replies[r][0][0] = word.split('Final rank: ')[1]\n",
    "        if 'final rank:' in word:\n",
    "            replies[r][0][0] = word.split('final rank: ')[1]\n",
    "        if 'final ranking:' in word:\n",
    "            replies[r][0][0] = word.split('final ranking: ')[1]\n",
    "        if 'Final Rank:' in word:\n",
    "            replies[r][0][0] = word.split('Final Rank: ')[1]\n",
    "        if 'Final Answer:' in word:\n",
    "            replies[r][0][0] = word.split('Final Answer: ')[1]\n",
    "        if 'Final ranking:' in word:\n",
    "            replies[r][0][0] = word.split('Final ranking: ')[1]\n",
    "    # look at the bad replies and see if they can be fixed\n",
    "    for r, reply in enumerate(replies):\n",
    "        if len(reply[0]) > 3:\n",
    "            replies[r][0] = reply[0][:3]\n",
    "    # remove any extra quotes around any of the replies\n",
    "    for r, reply in enumerate(replies):\n",
    "        for w, word in enumerate(reply[0]):\n",
    "            if word != '':\n",
    "                if word[0] == '\"' and word[-1] == '\"':\n",
    "                    replies[r][0][w] = word[1:-1]\n",
    "                if word[0] == \"'\" and word[-1] == \"'\":\n",
    "                    replies[r][0][w] = word[1:-1]\n",
    "    # get indices where theres not 3 words\n",
    "    # get indices where there's strings with more than 1 word in each of the 3 words\n",
    "    bad_reply_indices = [i for i, reply in enumerate(replies) if len(reply[0]) != 3 or len(reply[0][0].split()) > 1 or len(reply[0][1].split()) > 1 or len(reply[0][2].split()) > 1  or reply[0][0] == '' or reply[0][1] == '' or reply[0][2] == '']\n",
    "    # for bad_reply_index in bad_reply_indices:\n",
    "        # print('bad_reply_indices', bad_reply_index)\n",
    "        # print('replies[bad_reply_indices]', replies[bad_reply_index])\n",
    "    manual_bad_reply = ['won/spoil', '**so']\n",
    "    # manual_bad_reply_index = [i for i, reply in enumerate(replies) if len(reply[0]) >= 3 and reply[0][0] in manual_bad_reply or reply[0][1] in manual_bad_reply or reply[0][2] in manual_bad_reply]\n",
    "    manual_bad_reply_index = []\n",
    "    for i, reply in enumerate(replies):\n",
    "        if len(reply[0]) >= 3 and (reply[0][0] in manual_bad_reply or reply[0][1] in manual_bad_reply or reply[0][2] in manual_bad_reply):\n",
    "            # print('manual_bad_reply_index', i)\n",
    "            # print('replies[manual_bad_reply_index]', replies[i])\n",
    "            manual_bad_reply_index.append(i)\n",
    "    \n",
    "    # add to bad reply indices\n",
    "    bad_reply_indices += manual_bad_reply_index\n",
    "    # bad_reply_indices\n",
    "    # replies\n",
    "    # remove unecessary ** from either beginning or end of each word if it's there\n",
    "    for r, reply in enumerate(replies):\n",
    "        for w, word in enumerate(reply[0]):\n",
    "            if word != '':\n",
    "                if word[0] == '*' and word[1] == '*':\n",
    "                    replies[r][0][w] = word[2:]\n",
    "                if word[-1] == '*' and word[-2] == '*':\n",
    "                    replies[r][0][w] = word[:-2]\n",
    "    # replies\n",
    "    all_exps = replies\n",
    "    # data_name      = 'imdb'\n",
    "    model_name     = 'text_ann'\n",
    "    base_model_dir = './models/ClassWeighted/'\n",
    "    batch_size     = 1\n",
    "    num_test_samps = 100\n",
    "    model_dir, model_file_name = get_model_names(model_name, data_name, base_model_dir)\n",
    "    # Load dataset\n",
    "    loader_train, loader_val, loader_test = return_loaders(data_name=data_name, batch_size=batch_size, download=False)\n",
    "    \n",
    "    # X_train, y_train = torch.FloatTensor(loader_train.dataset.data), torch.LongTensor(loader_train.dataset.targets.to_numpy())\n",
    "    # X_val, y_val = torch.FloatTensor(loader_val.dataset.data), torch.LongTensor(loader_val.dataset.targets.to_numpy())\n",
    "    # X_test, y_test = torch.FloatTensor(loader_test.dataset.data), torch.LongTensor(loader_test.dataset.targets.to_numpy())\n",
    "    X_train = [data[0] for data in loader_train.dataset]\n",
    "    y_train = np.array([data[1] for data in loader_train.dataset])\n",
    "    X_val   = [data[0] for data in loader_val.dataset]\n",
    "    y_val   = np.array([data[1] for data in loader_val.dataset])\n",
    "    X_test  = [data[0] for data in loader_test.dataset]\n",
    "    y_test  = np.array([data[1] for data in loader_test.dataset])\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer, voc = get_tokenizer_and_vocab(X_train, y_train)\n",
    "    model = DefineModel(model_name, vocab_size=len(voc))\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_dir + model_file_name))\n",
    "    model.eval()\n",
    "    # model\n",
    "    X_test_lime = [data[0] for data in X_test]\n",
    "    X_train_lime = [data[0] for data in X_train]\n",
    "    \n",
    "    # keep the eval_inds of the test set\n",
    "    X_test_lime = [X_test_lime[i] for i in eval_inds]\n",
    "    # skip sentences with the following text\n",
    "    manual_skip_sentence = ['cinematography-if']\n",
    "    manual_skip_sentence_index = []\n",
    "    for i, sentence in enumerate(X_test_lime):\n",
    "        if manual_skip_sentence[0] in sentence:\n",
    "            # print('manual_skip_sentence_index', i)\n",
    "            # print('X_test_lime[manual_skip_sentence_index]', X_test_lime[i])\n",
    "            manual_skip_sentence_index.append(i)\n",
    "    # manual_skip_sentence_index\n",
    "    k = 3\n",
    "    experiment_id = getExperimentID()\n",
    "    PGI_AUC = calculateFaithfulnessAUC_text(X_test_lime, all_exps, model, 0, num_test_samps, k, do_pgu=False, do_random_baseline=False)\n",
    "    PGU_AUC = calculateFaithfulnessAUC_text(X_test_lime, all_exps, model, 0, num_test_samps, k, do_pgu=True, do_random_baseline=False)\n",
    "    print('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)))\n",
    "    print('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)))\n",
    "    \n",
    "    # save out to file\n",
    "    with open(f'outputs/TextFaithfulnessResults/{data_name}/faithfulness_{experiment_id}_{llm_name}_{data_name}_{model_name}_{prompt_strategy}_AUC-k{k}.txt', 'w') as f:\n",
    "        f.write('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)) + '\\n')\n",
    "        f.write('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)) + '\\n')\n",
    "    # print(PGI_AUC)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T18:54:06.883515Z",
     "start_time": "2024-06-14T18:54:03.561744Z"
    }
   },
   "id": "3ea5ddb8ce1cb1e",
   "execution_count": 1555
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e148661c66ee5a29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm_explainer",
   "language": "python",
   "display_name": "LLM_Explainer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
