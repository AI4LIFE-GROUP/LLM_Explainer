{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T07:51:13.615186Z",
     "start_time": "2024-03-29T07:51:13.608733Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a set of models on different OpenXAI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:55.206294Z",
     "start_time": "2024-03-28T22:25:55.203312Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# go up a directory\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:57.946044Z",
     "start_time": "2024-03-28T22:25:56.477839Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "import datetime\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders\n",
    "from openxai.ML_Models.LR.model import LogisticRegression\n",
    "import openxai.ML_Models.ANN.MLP as model_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:58.127480Z",
     "start_time": "2024-03-28T22:25:58.121107Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1194138b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.style.use('default')\n",
    "SEED = 3407\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:58.402042Z",
     "start_time": "2024-03-28T22:25:58.398909Z"
    }
   },
   "outputs": [],
   "source": [
    "def getExperimentID():\n",
    "    date_info = datetime.datetime.now()\n",
    "    testID    = '%d%02d%02d_%02d%02d' % (date_info.year, date_info.month, date_info.day, date_info.hour, date_info.minute)\n",
    "    return testID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:58.584195Z",
     "start_time": "2024-03-28T22:25:58.575010Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(model, loader_train, loader_test, ml_model, dir_name, learning_rate, epochs, dataset, exp_id, layer_info_str, use_class_weighting, modality='tabular'):\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = 'mps'\n",
    "    loaders = {'train': loader_train, 'test': loader_test}\n",
    "\n",
    "    if use_class_weighting: \n",
    "        # Compute class weights\n",
    "        class_counts  = torch.bincount(torch.tensor(loader_train.dataset.targets.to_numpy().astype(np.compat.long)))\n",
    "        total_samples = len(loader_train.dataset)\n",
    "        class_weights = total_samples / (class_counts.float())\n",
    "        class_weights = class_weights.to(device)\n",
    "\n",
    "    # model collector\n",
    "    best_auc_roc = 0\n",
    "\n",
    "    model  = model.to(device)\n",
    "\n",
    "    # declaring optimizer and loss\n",
    "    if use_class_weighting:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # training\n",
    "    for e in range(epochs):\n",
    "        print('Epoch {}/{}'.format(e, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "\n",
    "            all_preds  = []\n",
    "            all_labels = []\n",
    "            for i, input_tuple in enumerate(loaders[phase]):\n",
    "                if modality == 'tabular':\n",
    "                    inputs = input_tuple[0].float()\n",
    "                    labels = input_tuple[1]\n",
    "                elif modality == 'text':\n",
    "                    inputs = input_tuple[0]\n",
    "                    # text   = input_tuple[1]  # don't need for training!\n",
    "                    labels = input_tuple[2]\n",
    "                    \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).type(torch.long)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    y_pred = model(inputs)\n",
    "                    loss   = criterion(y_pred.float(), labels.long())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                all_preds.append(y_pred)\n",
    "                all_labels.append(labels)\n",
    "            # statistics\n",
    "            preds = torch.cat(all_preds, dim=0)\n",
    "            labels = torch.cat(all_labels, dim=0).cpu()\n",
    "            \n",
    "            raw_preds      = preds.data[:, 1]\n",
    "            raw_preds_np   = raw_preds.view(-1).cpu().numpy()\n",
    "            class_preds    = raw_preds >= 0.5\n",
    "            class_preds_np = class_preds.view(-1).long().cpu().numpy()\n",
    "            \n",
    "            epoch_loss     = loss.item()\n",
    "            epoch_acc      = accuracy_score(labels.numpy(), class_preds_np)\n",
    "            epoch_f1       = f1_score(labels.numpy(), class_preds_np)\n",
    "            epoch_auc_roc  = roc_auc_score(labels.numpy(), raw_preds_np)\n",
    "\n",
    "            print(f'{phase}: Loss: {epoch_loss:.4f} | F1-score: {epoch_f1:.4f} | AUC ROC: {epoch_auc_roc:.4f} | Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_auc_roc > best_auc_roc:\n",
    "                best_auc_roc = epoch_auc_roc\n",
    "                best_model_name = '{}_{}_{}_{}_{}_auc_roc_{:.2f}.pt'.format(exp_id, layer_info_str, dataset, ml_model, learning_rate, best_auc_roc)\n",
    "                print('new best model', e)\n",
    "                if use_class_weighting:\n",
    "                    fpth = 'models/ClassWeighted/' + dir_name\n",
    "                else:\n",
    "                    fpth = 'models/NotClassWeighted/' + dir_name\n",
    "                if not os.path.isdir(fpth):  # If folder doesn't exist, then create it.\n",
    "                    os.makedirs(fpth)\n",
    "                output_file_path = fpth + '/' + best_model_name\n",
    "                torch.save(model.state_dict(), output_file_path)\n",
    "\n",
    "    return best_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T22:25:58.747671Z",
     "start_time": "2024-03-28T22:25:58.743310Z"
    }
   },
   "outputs": [],
   "source": [
    "def PlotROC(output_dir, labels, preds, addtlNameInfo=''):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, preds)\n",
    "    auc         = roc_auc_score(labels, preds)\n",
    "    plt.figure(dpi=100)\n",
    "    plt.plot([0,1],[0,1], color='k')\n",
    "    plt.plot([0,0],[0,1], color='k', linestyle='dashed')\n",
    "    plt.plot([0,1],[1,1], color='k', linestyle='dashed')\n",
    "    plt.plot([1,1],[1,0], color='k', linestyle='dashed')\n",
    "    plt.plot([1,0],[0,0], color='k', linestyle='dashed')\n",
    "    plt.plot(fpr, tpr, color='b', label=\"AUC: \" + str(round(auc,2)))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('ROC Curve - ' + addtlNameInfo)\n",
    "    plt.savefig(output_dir + 'ROC_'+addtlNameInfo+'.png', bbox_inches='tight')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T23:00:46.231966Z",
     "start_time": "2024-03-28T23:00:46.223416Z"
    }
   },
   "outputs": [],
   "source": [
    "def EvaluateNetwork(best_model_name, model_name, dim_per_layer, activation_per_layer, loader_train, loader_val, \n",
    "                    use_class_weighting, modality='tabular'):\n",
    "    print('Evaluating', best_model_name)\n",
    "\n",
    "    model = DefineModel(model_name, dim_per_layer, activation_per_layer)\n",
    "\n",
    "    if use_class_weighting:\n",
    "        fpth = 'models/ClassWeighted/'\n",
    "    else:\n",
    "        fpth = 'models/NotClassWeighted/'\n",
    "        \n",
    "    model.load_state_dict(torch.load(fpth + model_name.upper() + '/' + best_model_name))\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = 'mps'\n",
    "    model  = model.to(device)\n",
    "    model.eval()   # Set model to evaluation mode\n",
    "\n",
    "    all_preds  = []\n",
    "    all_labels = []\n",
    "    for i, input_tuple in enumerate(loader_val):\n",
    "        if modality == 'tabular':\n",
    "            inputs = input_tuple[0].float()\n",
    "            labels = input_tuple[1]\n",
    "        elif modality == 'text':\n",
    "            inputs = input_tuple[0]\n",
    "            # text   = input_tuple[1]  # don't need for evaluation!\n",
    "            labels = input_tuple[2]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).type(torch.long)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            y_pred = model(inputs.float())\n",
    "        all_preds.append(y_pred)\n",
    "        all_labels.append(labels)\n",
    "    # statistics\n",
    "    preds  = torch.cat(all_preds, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0).cpu()\n",
    "    \n",
    "    raw_preds      = preds.data[:, 1]\n",
    "    raw_preds_np   = raw_preds.view(-1).cpu().numpy()\n",
    "    class_preds    = raw_preds >= 0.5\n",
    "    class_preds_np = class_preds.view(-1).long().cpu().numpy()\n",
    "    \n",
    "    total_acc     = accuracy_score(labels.numpy(), class_preds_np)\n",
    "    total_f1      = f1_score(labels.numpy(), class_preds_np)\n",
    "    total_auc_roc = roc_auc_score(labels.numpy(), raw_preds_np)\n",
    "    \n",
    "    y_preds_test = []\n",
    "    for i, input_tuple in enumerate(loader_val):\n",
    "        inputs = input_tuple[0].float().to(device)\n",
    "        y_pred = model(inputs).data\n",
    "        y_pred = y_pred[:, 1]# >= 0.5  # True if bigger than 0.5\n",
    "        y_preds_test.append(y_pred.cpu())  # convert true and false to 1 and 0, respectively.\n",
    "    y_preds_test_flat = np.array([item for sublist in y_preds_test for item in sublist])\n",
    "\n",
    "    y_preds_train = []\n",
    "    for i, input_tuple in enumerate(loader_train):\n",
    "        inputs = input_tuple[0].float().to(device)\n",
    "        y_pred = model(inputs).data\n",
    "        y_pred = y_pred[:, 1]# >= 0.5  # True if bigger than 0.5\n",
    "        y_preds_train.append(y_pred.cpu())  # convert true and false to 1 and 0, respectively.\n",
    "    y_preds_train_flat = np.array([item for sublist in y_preds_train for item in sublist])\n",
    "\n",
    "    return total_acc, total_f1, total_auc_roc, y_preds_test_flat, y_preds_train_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T23:00:46.848682Z",
     "start_time": "2024-03-28T23:00:46.845386Z"
    }
   },
   "outputs": [],
   "source": [
    "def PlotConfusionMatrix(labels, preds, num_of_classes, output_dir, addtlNameInfo=''):\n",
    "    preds      = preds >= 0.5\n",
    "    conf_mat   = confusion_matrix(labels, preds, normalize=None)\n",
    "    pathToSave = os.path.join(output_dir + 'ConfusionMatrix_'+addtlNameInfo+'.png')\n",
    "    df_cm      = pd.DataFrame(conf_mat, range(num_of_classes), range(num_of_classes))\n",
    "    \n",
    "    plt.figure(figsize=(12, 9), dpi=100)\n",
    "    plt.title('Confusion Matrix - ' + addtlNameInfo)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    # sn.set(font_scale=2) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 18}, cmap=\"Blues\", fmt='d') # font size\n",
    "    plt.savefig(pathToSave, format = 'png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T23:00:47.044359Z",
     "start_time": "2024-03-28T23:00:47.036834Z"
    }
   },
   "outputs": [],
   "source": [
    "def SaveModelInfo(model_name, data_name, model, exp_id, epochs, learning_rate, X_train, y_train, X_val, y_val, X_test, y_test, total_f1, total_acc, total_auc_roc, loader_val, y_preds_test_flat, loader_train, y_preds_train_flat, layer_info_str, use_class_weighting, dim_per_layer='', activation_per_layer=''):\n",
    "    # Save to .txt\n",
    "    if use_class_weighting:\n",
    "        output_dir = 'models/ClassWeighted/' + model_name.upper() + '/'\n",
    "    else:\n",
    "        output_dir = 'models/NotClassWeighted/' + model_name.upper() + '/'\n",
    "    file_name  =  exp_id + '_' + layer_info_str + model_name.upper() + '_' + data_name + '_summary'\n",
    "\n",
    "    fpth = os.path.join(output_dir, file_name+'.txt')\n",
    "    paramTxt = open(fpth, 'w')\n",
    "\n",
    "    paramTxt.write(file_name)\n",
    "    paramTxt.write('Hyperparameters')\n",
    "    paramTxt.write('exp_id:\\t' + exp_id + '\\n')\n",
    "    paramTxt.write('data_name:\\t' + data_name + '\\n')\n",
    "    paramTxt.write('model_name:\\t' + model_name + '\\n')\n",
    "    paramTxt.write('epochs:\\t\\t' + str(epochs) + '\\n')\n",
    "    paramTxt.write('learning_rate:\\t' + str(learning_rate) + '\\n\\n')\n",
    "    paramTxt.write('X_train.shape:\\t' + str(X_train.shape) + '\\n')\n",
    "    paramTxt.write('y_train.shape:\\t' + str(y_train.shape) + '\\n\\n')\n",
    "    paramTxt.write('X_val.shape:\\t' + str(X_val.shape) + '\\n')\n",
    "    paramTxt.write('y_val.shape:\\t' + str(y_val.shape) + '\\n\\n')\n",
    "    paramTxt.write('X_test.shape:\\t' + str(X_test.shape) + '\\n')\n",
    "    paramTxt.write('y_test.shape:\\t' + str(y_test.shape) + '\\n\\n')\n",
    "    paramTxt.write('dim_per_layer:\\t' + str(dim_per_layer) + '\\n')\n",
    "    if 'ann' in model_name:\n",
    "        paramTxt.write('activation_per_layer:\\t' + str(activation_per_layer) + '\\n')\n",
    "    paramTxt.write('\\nF1-score: '+ str(round(total_f1,4)) +' | Accuracy: ' + str(round(total_acc, 4)) + ' | AUC-ROC: ' + str(round(total_auc_roc, 4)) + '\\n\\n')\n",
    "    paramTxt.write(\"Proportion of ones in test set: \" + str(round(np.mean(loader_val.dataset.targets),3)) + '\\n')\n",
    "    paramTxt.write(\"Proportion of ones predicted in test set: \" + str(round(np.mean(y_preds_test_flat),3)) + '\\n')\n",
    "    paramTxt.write(\"Proportion of ones in train set: \" + str(round(np.mean(loader_train.dataset.targets),3)) + '\\n')\n",
    "    paramTxt.write(\"Proportion of ones predicted in train set: \" + str(round(np.mean(y_preds_train_flat),3)) + '\\n')\n",
    "    paramTxt.write('\\nArchitecture:\\n')\n",
    "    paramTxt.write(str(model.__dict__['_modules']))\n",
    "    paramTxt.close()\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    PlotROC(fpth, y_val, y_preds_test_flat, addtlNameInfo = model_name.upper() + '_' + data_name)\n",
    "    \n",
    "    PlotConfusionMatrix(y_val, y_preds_test_flat, 2, output_dir, addtlNameInfo = model_name.upper() + '_' + data_name)\n",
    "\n",
    "    print(f'F1-score: {total_f1:.4f} | Accuracy: {total_acc:.4f} | AUC-ROC: {total_auc_roc:.4f}')\n",
    "    print(\"Proportion of ones in test set:\", round(np.mean(loader_val.dataset.targets),3))\n",
    "    print(\"Proportion of ones predicted in test set: \", round(np.mean(y_preds_test_flat),3))\n",
    "    print(\"Proportion of ones in train set:\", round(np.mean(loader_train.dataset.targets),3))\n",
    "    print(\"Proportion of ones predicted in train set: \", round(np.mean(y_preds_train_flat),3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T23:00:47.206902Z",
     "start_time": "2024-03-28T23:00:47.204455Z"
    }
   },
   "outputs": [],
   "source": [
    "def DefineModel(model_name, dim_per_layer=None, activation_per_layer=None):\n",
    "    input_size = loader_train.dataset.get_number_of_features()\n",
    "\n",
    "    if 'ann' in model_name:\n",
    "        dim_per_layer = [input_size] + dim_per_layer\n",
    "        model         = model_MLP.MLP(dim_per_layer, activation_per_layer)\n",
    "    elif model_name == 'lr':\n",
    "        dim_per_layer = [input_size] + dim_per_layer\n",
    "        model         = LogisticRegression(dim_per_layer[0], dim_per_layer[1])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T01:38:35.640880Z",
     "start_time": "2024-03-29T01:38:35.629600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#get number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-29T01:39:23.942521Z",
     "start_time": "2024-03-29T01:39:23.890086Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: ann_xxl on credit\n",
      "ann_xxl\n",
      "num params 180874\n",
      "Epoch 0/14\n",
      "----------\n",
      "train: Loss: 0.5687 | F1-score: 0.8725 | AUC ROC: 0.7642 | Accuracy: 0.7806\n",
      "test: Loss: 0.5859 | F1-score: 0.9021 | AUC ROC: 0.8001 | Accuracy: 0.8268\n",
      "new best model 0\n",
      "Epoch 1/14\n",
      "----------\n",
      "train: Loss: 0.5138 | F1-score: 0.8517 | AUC ROC: 0.8083 | Accuracy: 0.7512\n",
      "test: Loss: 0.5870 | F1-score: 0.9210 | AUC ROC: 0.8029 | Accuracy: 0.8575\n",
      "new best model 1\n",
      "Epoch 2/14\n",
      "----------\n",
      "train: Loss: 0.4974 | F1-score: 0.8552 | AUC ROC: 0.8125 | Accuracy: 0.7563\n",
      "test: Loss: 0.6132 | F1-score: 0.9451 | AUC ROC: 0.8006 | Accuracy: 0.8981\n",
      "Epoch 3/14\n",
      "----------\n",
      "train: Loss: 0.4879 | F1-score: 0.8506 | AUC ROC: 0.8107 | Accuracy: 0.7496\n",
      "test: Loss: 0.5803 | F1-score: 0.8987 | AUC ROC: 0.8056 | Accuracy: 0.8215\n",
      "new best model 3\n",
      "Epoch 4/14\n",
      "----------\n",
      "train: Loss: 0.6239 | F1-score: 0.8535 | AUC ROC: 0.8164 | Accuracy: 0.7539\n",
      "test: Loss: 0.5956 | F1-score: 0.9138 | AUC ROC: 0.8042 | Accuracy: 0.8457\n",
      "Epoch 5/14\n",
      "----------\n",
      "train: Loss: 0.5213 | F1-score: 0.8567 | AUC ROC: 0.8131 | Accuracy: 0.7584\n",
      "test: Loss: 0.5814 | F1-score: 0.9139 | AUC ROC: 0.8075 | Accuracy: 0.8459\n",
      "new best model 5\n",
      "Epoch 6/14\n",
      "----------\n",
      "train: Loss: 0.6248 | F1-score: 0.8591 | AUC ROC: 0.8153 | Accuracy: 0.7620\n",
      "test: Loss: 0.5803 | F1-score: 0.9394 | AUC ROC: 0.8038 | Accuracy: 0.8884\n",
      "Epoch 7/14\n",
      "----------\n",
      "train: Loss: 0.4948 | F1-score: 0.8722 | AUC ROC: 0.8109 | Accuracy: 0.7813\n",
      "test: Loss: 0.5814 | F1-score: 0.9204 | AUC ROC: 0.8062 | Accuracy: 0.8565\n",
      "Epoch 8/14\n",
      "----------\n",
      "train: Loss: 0.4720 | F1-score: 0.8575 | AUC ROC: 0.8142 | Accuracy: 0.7598\n",
      "test: Loss: 0.5897 | F1-score: 0.9115 | AUC ROC: 0.8078 | Accuracy: 0.8420\n",
      "new best model 8\n",
      "Epoch 9/14\n",
      "----------\n",
      "train: Loss: 0.4875 | F1-score: 0.8711 | AUC ROC: 0.8164 | Accuracy: 0.7797\n",
      "test: Loss: 0.5922 | F1-score: 0.9124 | AUC ROC: 0.8061 | Accuracy: 0.8434\n",
      "Epoch 10/14\n",
      "----------\n",
      "train: Loss: 0.5638 | F1-score: 0.8674 | AUC ROC: 0.8180 | Accuracy: 0.7743\n",
      "test: Loss: 0.5913 | F1-score: 0.9271 | AUC ROC: 0.8080 | Accuracy: 0.8676\n",
      "new best model 10\n",
      "Epoch 11/14\n",
      "----------\n",
      "train: Loss: 0.5187 | F1-score: 0.8725 | AUC ROC: 0.8187 | Accuracy: 0.7819\n",
      "test: Loss: 0.6003 | F1-score: 0.8919 | AUC ROC: 0.8073 | Accuracy: 0.8108\n",
      "Epoch 12/14\n",
      "----------\n",
      "train: Loss: 0.5052 | F1-score: 0.8586 | AUC ROC: 0.8184 | Accuracy: 0.7614\n",
      "test: Loss: 0.5851 | F1-score: 0.8975 | AUC ROC: 0.8087 | Accuracy: 0.8196\n",
      "new best model 12\n",
      "Epoch 13/14\n",
      "----------\n",
      "train: Loss: 0.5008 | F1-score: 0.8582 | AUC ROC: 0.8214 | Accuracy: 0.7608\n",
      "test: Loss: 0.5935 | F1-score: 0.9169 | AUC ROC: 0.8082 | Accuracy: 0.8507\n",
      "Epoch 14/14\n",
      "----------\n",
      "train: Loss: 0.5450 | F1-score: 0.8750 | AUC ROC: 0.8179 | Accuracy: 0.7856\n",
      "test: Loss: 0.5899 | F1-score: 0.9238 | AUC ROC: 0.8094 | Accuracy: 0.8621\n",
      "new best model 14\n",
      "Evaluating 20240523_1903_512_256_128_64_32_16_8_2__credit_ann_xxl_0.001_auc_roc_0.81.pt\n",
      "F1-score: 0.9238 | Accuracy: 0.8621 | AUC-ROC: 0.8094\n",
      "Proportion of ones in test set: 0.951\n",
      "Proportion of ones predicted in test set:  0.816\n",
      "Proportion of ones in train set: 0.95\n",
      "Proportion of ones predicted in train set:  0.727 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_per_layer_per_MLP = {'ann_s':  [16, 2],\n",
    "                         'ann_m':  [32, 16, 2],\n",
    "                         'ann_l':  [64, 32, 16, 2],\n",
    "                         'ann_xl': [256, 128, 64, 32, 16, 2],\n",
    "                         'ann_xxl': [512, 256, 128, 64, 32, 16, 8, 2], \n",
    "                         'lr':     [2]\n",
    "                         }  # dimension for each layer for each network to train, ignoring input layer size\n",
    "activation_per_layer_per_MLP = {'ann_s':  [nn.ReLU(), None],\n",
    "                                'ann_m':  [nn.ReLU(), nn.ReLU(), None],\n",
    "                                'ann_l':  [nn.ReLU(), nn.ReLU(), nn.ReLU(), None],\n",
    "                                'ann_xl': [nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), None],\n",
    "                                'ann_xxl': [nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), None],\n",
    "                                'lr':     [None]\n",
    "                                } # ignore input layer size\n",
    "\n",
    "data_names          = ['credit'] #'adult', 'compas', 'credit', 'german', 'heloc', 'credit']#, 'rcdv', 'student'] # 'lending-club',\n",
    "model_names         = ['ann_xxl'] #, 'ann_xxl']#['lr', 'ann_s', 'ann_m', 'ann_l', 'ann_xl']\n",
    "epochs              = 15\n",
    "learning_rate       = 0.001\n",
    "use_class_weighting = True\n",
    "\n",
    "for data_name in data_names:\n",
    "    if data_name == 'beauty':\n",
    "        modality = 'text'\n",
    "    else:\n",
    "        modality = 'tabular'\n",
    "    for model_name in model_names:\n",
    "        print(\"Training:\", model_name, \"on\", data_name)\n",
    "        # Define hyperparameters\n",
    "        exp_id   = getExperimentID()\n",
    "        dir_name = model_name.upper()\n",
    "\n",
    "        # Get the data for training and evaluation\n",
    "        if data_name in ['compas', 'blood', 'beauty']:\n",
    "            download_data = False\n",
    "        else:\n",
    "            download_data = True\n",
    "\n",
    "        loader_train, loader_val, loader_test = return_loaders(data_name=data_name, download=download_data, batch_size=256, scaler='minmax')\n",
    "\n",
    "        X_train, y_train = loader_train.dataset.data, loader_train.dataset.targets.to_numpy()\n",
    "        X_val, y_val     = loader_val.dataset.data, loader_val.dataset.targets.to_numpy()\n",
    "        X_test, y_test   = loader_test.dataset.data, loader_test.dataset.targets.to_numpy()\n",
    "\n",
    "        # Define the model\n",
    "        layer_info_str = '' #empty for lr, fill for ann\n",
    "        for d in dim_per_layer_per_MLP[model_name]:\n",
    "            layer_info_str += str(d) + '_'\n",
    "\n",
    "        model = DefineModel(model_name, dim_per_layer_per_MLP[model_name], activation_per_layer_per_MLP[model_name])\n",
    "        print(model_name)\n",
    "        print('num params', count_parameters(model))\n",
    "        # \n",
    "        # Train the model\n",
    "        best_model_name = training(model, loader_train, loader_val, model_name, dir_name,\n",
    "                                   learning_rate, epochs, data_name, exp_id, layer_info_str, use_class_weighting, modality=modality)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        total_acc, total_f1, total_auc_roc, y_preds_test_flat, y_preds_train_flat = \\\n",
    "            EvaluateNetwork(best_model_name, model_name, dim_per_layer_per_MLP[model_name],\n",
    "                            activation_per_layer_per_MLP[model_name], loader_train, loader_val, use_class_weighting, modality=modality)\n",
    "        \n",
    "        # Save model info\n",
    "        SaveModelInfo(model_name, data_name, model, exp_id, epochs, learning_rate,\n",
    "                      X_train, y_train, X_val, y_val, X_test, y_test, total_f1, total_acc, total_auc_roc,\n",
    "                      loader_val, y_preds_test_flat, loader_train, y_preds_train_flat, layer_info_str,\n",
    "                      use_class_weighting, dim_per_layer_per_MLP[model_name], activation_per_layer_per_MLP[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets:\n",
    "# LR, ANN_S, ANN_M, ANN_L Compas\n",
    "# LR, ANN_S, ANN_M, ANN_L german\n",
    "# LR, ANN_S, ANN_M, ANN_L heloc\n",
    "# LR, ANN_S, ANN_M, ANN_L adult income\n",
    "# synthetic dataset\n",
    "# LR, ANN_S, ANN_M, ANN_L give me some credit (credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:46:53.273803Z",
     "start_time": "2023-06-29T16:46:53.195361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed values saved to parsed_values.csv.\n",
      "Parsed values saved to parsed_values.csv.\n",
      "Parsed values saved to parsed_values.csv.\n"
     ]
    }
   ],
   "source": [
    "# gather all the scores and save them\n",
    "import os\n",
    "import csv\n",
    "\n",
    "for model_name in model_names:\n",
    "    folder_path = './models/ClassWeighted/' + model_name.upper() +'/' # Replace with the actual folder path containing the text files\n",
    "    output_file = 'parsed_values.csv'  # Replace with the desired output file name\n",
    "        \n",
    "    # Define the fieldnames for the CSV file\n",
    "    fieldnames = ['data_name', 'F1-score', 'Accuracy', 'AUC-ROC', 'Proportion of ones in test set',\n",
    "                  'Proportion of ones predicted in test set', 'Proportion of ones in train set',\n",
    "                  'Proportion of ones predicted in train set']\n",
    "    \n",
    "    # Create a list to store the parsed values\n",
    "    parsed_values = []\n",
    "    \n",
    "    # Iterate over the text files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Process only text files\n",
    "            with open(os.path.join(folder_path, filename), 'r') as file:\n",
    "                content = file.read()\n",
    "    \n",
    "                # Extract the required values using string manipulation or regular expressions\n",
    "                data_name = content.split('data_name:')[1].split('\\n')[0]\n",
    "                f1_score = content.split('F1-score: ')[1].split(' |')[0]\n",
    "                accuracy = content.split('Accuracy: ')[1].split(' |')[0]\n",
    "                auc_roc = content.split('AUC-ROC: ')[1].split('\\n')[0]\n",
    "                ones_test = content.split('Proportion of ones in test set: ')[1].split('\\n')[0]\n",
    "                ones_predicted_test = content.split('Proportion of ones predicted in test set: ')[1].split('\\n')[0]\n",
    "                ones_train = content.split('Proportion of ones in train set: ')[1].split('\\n')[0]\n",
    "                ones_predicted_train = content.split('Proportion of ones predicted in train set: ')[1].split('\\n')[0]\n",
    "    \n",
    "                # Append the parsed values to the list\n",
    "                parsed_values.append({\n",
    "                    'data_name': data_name,\n",
    "                    'F1-score': f1_score,\n",
    "                    'Accuracy': accuracy,\n",
    "                    'AUC-ROC': auc_roc,\n",
    "                    'Proportion of ones in test set': ones_test,\n",
    "                    'Proportion of ones predicted in test set': ones_predicted_test,\n",
    "                    'Proportion of ones in train set': ones_train,\n",
    "                    'Proportion of ones predicted in train set': ones_predicted_train\n",
    "                })\n",
    "    \n",
    "            # sort the rows by data_name\n",
    "            parsed_values = sorted(parsed_values, key=lambda k: k['data_name'])\n",
    "            # Write the parsed values to the CSV file\n",
    "            with open(folder_path+output_file, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(parsed_values)\n",
    "            \n",
    "            print(f\"Parsed values saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:20:26.642727Z",
     "start_time": "2023-06-29T16:20:26.604427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"20240328_1621_256_128_64_32_16_2_ANN_XL_compas_summaryHyperparametersexp_id:\\t20240328_1621\\ndata_name:\\tcompas\\nmodel_name:\\tann_xl\\nepochs:\\t\\t100\\nlearning_rate:\\t0.001\\n\\nX_train.shape:\\t(3950, 6)\\ny_train.shape:\\t(3950,)\\n\\nX_val.shape:\\t(987, 6)\\ny_val.shape:\\t(987,)\\n\\nX_test.shape:\\t(1235, 6)\\ny_test.shape:\\t(1235,)\\n\\ndim_per_layer:\\t[256, 128, 64, 32, 16, 2]\\nactivation_per_layer:\\t[ReLU(), ReLU(), ReLU(), ReLU(), ReLU(), None]\\n\\nF1-score: 0.8458 | Accuracy: 0.771 | AUC-ROC: 0.828\\n\\nProportion of ones in test set: 0.785\\nProportion of ones predicted in test set: 0.683\\nProportion of ones in train set: 0.822\\nProportion of ones predicted in train set: 0.63\\n\\nArchitecture:\\nOrderedDict([('layers', ModuleList(\\n  (0): Linear(in_features=6, out_features=256, bias=True)\\n  (1): ReLU()\\n  (2): Linear(in_features=256, out_features=128, bias=True)\\n  (3): ReLU()\\n  (4): Linear(in_features=128, out_features=64, bias=True)\\n  (5): ReLU()\\n  (6): Linear(in_features=64, out_features=32, bias=True)\\n  (7): ReLU()\\n  (8): Linear(in_features=32, out_features=16, bias=True)\\n  (9): ReLU()\\n  (10): Linear(in_features=16, out_features=2, bias=True)\\n))])\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
