{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T01:37:38.122600Z",
     "start_time": "2024-06-12T01:37:38.032826Z"
    }
   },
   "id": "d7bd8991204843a8",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# go up a directory\n",
    "import os\n",
    "os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:58:24.685594Z",
     "start_time": "2024-06-13T07:58:24.683723Z"
    }
   },
   "id": "b9d6c5718d92996b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:58:25.050959Z",
     "start_time": "2024-06-13T07:58:25.048587Z"
    }
   },
   "outputs": [],
   "source": [
    "from openxai.Explainer import Explainer\n",
    "import torch\n",
    "from utils import get_model_names\n",
    "from openxai.dataloader import return_loaders, get_tokenizer_and_vocab\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "from openxai.ML_Models.LR.model import LogisticRegression\n",
    "import openxai.ML_Models.ANN.MLP as model_MLP\n",
    "import openxai.ML_Models.ANN.Text_MLP as model_MLP_Text\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def getExperimentID():\n",
    "    date_info = datetime.datetime.now()\n",
    "    testID    = '%d%02d%02d_%02d%02d' % (date_info.year, date_info.month, date_info.day, date_info.hour, date_info.minute)\n",
    "    return testID"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:58:25.476669Z",
     "start_time": "2024-06-13T07:58:25.474469Z"
    }
   },
   "id": "e4d69df6c20bfaa8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_name = 'yelp'\n",
    "model_name = 'text_ann'\n",
    "base_model_dir = './models/ClassWeighted/'\n",
    "model_dir, model_file_name = get_model_names(model_name, data_name, base_model_dir)\n",
    "num_test_samps = 100\n",
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:41.650969Z",
     "start_time": "2024-06-13T07:59:41.648329Z"
    }
   },
   "id": "1f3ec9349c0139be",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1721\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "loader_train, loader_val, loader_test = return_loaders(data_name=data_name, batch_size=batch_size, download=False)\n",
    "\n",
    "# Get the data\n",
    "X_train = [data[0] for data in loader_train.dataset]\n",
    "y_train = np.array([data[1] for data in loader_train.dataset])\n",
    "X_val   = [data[0] for data in loader_val.dataset]\n",
    "y_val   = np.array([data[1] for data in loader_val.dataset])\n",
    "X_test  = [data[0] for data in loader_test.dataset]\n",
    "y_test  = np.array([data[1] for data in loader_test.dataset])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:42.055461Z",
     "start_time": "2024-06-13T07:59:42.044693Z"
    }
   },
   "id": "90a6e2bb3e915984",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def DefineModel(model_name, dim_per_layer=None, activation_per_layer=None, vocab_size=None, embed_dim=None, num_class=None):\n",
    "    if 'text_ann' in model_name:\n",
    "        # model = model_MLP_Text.Text_MLP(vocab_size, embed_dim, num_class)\n",
    "        model = model_MLP_Text.Text_MLP(vocab_size)\n",
    "    else:\n",
    "        input_size = loader_train.dataset.get_number_of_features()\n",
    "        if 'ann' in model_name:\n",
    "            dim_per_layer = [input_size] + dim_per_layer\n",
    "            model         = model_MLP.MLP(dim_per_layer, activation_per_layer)\n",
    "        elif model_name == 'lr':\n",
    "            dim_per_layer = [input_size] + dim_per_layer\n",
    "            model         = LogisticRegression(dim_per_layer[0], dim_per_layer[1])\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:42.412891Z",
     "start_time": "2024-06-13T07:59:42.409988Z"
    }
   },
   "id": "416b412f90ee1da0",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1721\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer, voc = get_tokenizer_and_vocab(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:42.724440Z",
     "start_time": "2024-06-13T07:59:42.717668Z"
    }
   },
   "id": "4ec4d816299066d2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Text_MLP(\n  (embeddings): TokenEmbedding(\n    (embedding): Embedding(1721, 8)\n  )\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.3, inplace=False)\n  )\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0): TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n        )\n        (linear1): Linear(in_features=8, out_features=32, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=32, out_features=8, bias=True)\n        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (linear): Linear(in_features=8, out_features=2, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DefineModel(model_name, vocab_size=len(voc))\n",
    "\n",
    "model.load_state_dict(torch.load(model_dir + model_file_name))\n",
    "model.eval()\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:43.046051Z",
     "start_time": "2024-06-13T07:59:43.024381Z"
    }
   },
   "id": "4d95d2b973667d4f",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# LIME\n",
    "kernel_width           = 0.75\n",
    "std_LIME               = 0.1\n",
    "mode                   = 'text'\n",
    "sample_around_instance = True\n",
    "n_samples_LIME         = 1000\n",
    "discretize_continuous  = False\n",
    "\n",
    "# grad\n",
    "absolute_value = True\n",
    "\n",
    "# Smooth grad\n",
    "n_samples_SG = 100#16\n",
    "std_SG       = 0.005\n",
    "\n",
    "# Integrated gradients\n",
    "method             = 'gausslegendre'\n",
    "multiply_by_inputs = False\n",
    "n_steps            = 50#16\n",
    "\n",
    "#SHAP\n",
    "n_samples = 500#16"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:43.725648Z",
     "start_time": "2024-06-13T07:59:43.722953Z"
    }
   },
   "id": "78375a1d17697289",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "param_dict_lime = dict()\n",
    "param_dict_lime['dataset_tensor']         = None\n",
    "param_dict_lime['kernel_width']           = kernel_width\n",
    "param_dict_lime['std']                    = std_LIME\n",
    "param_dict_lime['mode']                   = mode\n",
    "param_dict_lime['sample_around_instance'] = sample_around_instance\n",
    "param_dict_lime['n_samples']              = n_samples_LIME\n",
    "param_dict_lime['discretize_continuous']  = discretize_continuous\n",
    "param_dict_lime['categorical_features']   = None\n",
    "\n",
    "param_dict_grad                   = dict()\n",
    "param_dict_grad['absolute_value'] = absolute_value\n",
    "\n",
    "param_dict_sg                       = dict()\n",
    "param_dict_sg['n_samples']          = n_samples_SG\n",
    "param_dict_sg['standard_deviation'] = std_SG\n",
    "\n",
    "param_dict_ig                       = dict()\n",
    "param_dict_ig['method']             = method\n",
    "param_dict_ig['multiply_by_inputs'] = multiply_by_inputs\n",
    "param_dict_ig['baseline']           = None #torch.mean(X_train, dim=0).reshape(1, -1).float()\n",
    "param_dict_ig['n_steps']            = n_steps\n",
    "\n",
    "param_dict_shap              = dict()\n",
    "param_dict_shap['n_samples'] = n_samples\n",
    "\n",
    "param_dicts = {'lime': param_dict_lime, 'grad': param_dict_grad, 'sg': param_dict_sg, 'ig': param_dict_ig,\n",
    "               'shap': param_dict_shap, 'itg': dict(), 'random': dict()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:44.357052Z",
     "start_time": "2024-06-13T07:59:44.353662Z"
    }
   },
   "id": "6309d4221e45d6f4",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test_lime = [data[0] for data in X_test]\n",
    "X_train_lime = [data[0] for data in X_train]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T07:59:44.997997Z",
     "start_time": "2024-06-13T07:59:44.995835Z"
    }
   },
   "id": "96c0514007073be",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculateFaithfulnessAUC_text(test_sentences, explanations, text_classifier, min_idx, max_idx, max_k, do_pgu=False, do_random_baseline=False):\n",
    "    PG_AUC = []\n",
    "    for index, test_sentence in enumerate(test_sentences):\n",
    "        if len(tokenizer(test_sentence)) <= max_k + 2:\n",
    "            print(f'Less than {max_k} tokens in sentence. Skipping the sentence:', test_sentence)\n",
    "            continue\n",
    "        if index == max_idx:\n",
    "            break\n",
    "        if max_k > 1:\n",
    "            auc_x = np.arange(max_k) / (max_k - 1)\n",
    "        PG = []\n",
    "        for top_k in range(1, max_k + 1):\n",
    "            PG.append(\n",
    "                PG_words(test_sentence, explanations[index][0][:top_k], text_classifier, do_pgu, do_random_baseline)) \n",
    "        if max_k > 1:\n",
    "            PG_AUC.append(auc(auc_x, PG))\n",
    "        else:\n",
    "            PG_AUC.append(PG)\n",
    "\n",
    "    return PG_AUC\n",
    "\n",
    "def PG_words(test_sentence, topk_exp_words, text_classifier, do_pgu, do_random_baseline=False):\n",
    "    tokenized_input = torch.tensor([voc[t] for t in tokenizer(test_sentence)])\n",
    "    with torch.no_grad():\n",
    "        pred_original = text_classifier(tokenized_input.unsqueeze(0))\n",
    "\n",
    "    if do_random_baseline:\n",
    "        # Create a mask where each element is True if it is NOT in values_to_remove\n",
    "        mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "        if not do_pgu:\n",
    "            while mask.sum() == 0:\n",
    "                mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "        else:\n",
    "            while mask.sum() == len(tokenized_input):\n",
    "                mask = torch.rand(tokenized_input.shape) > 0.5\n",
    "    else:\n",
    "        # find the indices where the topk words are in the sentence\n",
    "        voc_values_to_remove = []\n",
    "        for word in topk_exp_words:\n",
    "            top_k_idx_to_remove = voc[tokenizer(word)[0]]\n",
    "            voc_values_to_remove.append(top_k_idx_to_remove)\n",
    "        \n",
    "        # Create a mask where each element is True if it is NOT in values_to_remove\n",
    "        mask = ~torch.isin(tokenized_input, torch.tensor(voc_values_to_remove))\n",
    "    \n",
    "    if do_pgu:\n",
    "        # flip the mask\n",
    "        mask = ~mask\n",
    "    \n",
    "    # Apply the mask to get the filtered tensor\n",
    "    filtered_tokenized_input = tokenized_input[mask]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_removed = text_classifier(filtered_tokenized_input.unsqueeze(0))\n",
    "        \n",
    "    PG = torch.abs(pred_original.squeeze() - pred_removed.squeeze())[0] # [0] - take first class. assume binary classifier (this is what openxai does) \n",
    "    return PG"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:32.019304Z",
     "start_time": "2024-06-13T08:05:32.012597Z"
    }
   },
   "id": "7675234677b69338",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def classifier_fn(perturbed_texts):\n",
    "    all_tokenized_sentences = []\n",
    "    for text in perturbed_texts:\n",
    "        temp_texts = []\n",
    "        for t in tokenizer(text):\n",
    "            temp_texts.append(voc[t])\n",
    "        all_tokenized_sentences.append(temp_texts)\n",
    "    \n",
    "    max_len = max([len(tokens) for tokens in all_tokenized_sentences])\n",
    "    for sentence in all_tokenized_sentences:\n",
    "        while len(sentence) < max_len:\n",
    "            sentence.append(0) # pad token\n",
    "    all_tokenized_sentences = [torch.tensor(sentence) for sentence in all_tokenized_sentences]\n",
    "    inputs = torch.stack(all_tokenized_sentences)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(inputs)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:33.170852Z",
     "start_time": "2024-06-13T08:05:33.167920Z"
    }
   },
   "id": "c03df09da2793de",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "k = 3\n",
    "experiment_id = getExperimentID()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:33.459197Z",
     "start_time": "2024-06-13T08:05:33.457378Z"
    }
   },
   "id": "7045c6fcf9107041",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo: lime16\n",
      "text_instance:  the presentation of the food was awful.\n",
      "text_instance:  Worst food/service I've had in a while.\n",
      "text_instance:  Never again will I be dining at this place!\n",
      "text_instance:  I guess maybe we went on an off night but it was disgraceful.\n",
      "text_instance:  As a sushi lover avoid this place by all means.\n",
      "text_instance:  The ambiance isn't much better.\n",
      "text_instance:  This hole in the wall has great Mexican street tacos, and friendly staff.\n",
      "text_instance:  If the food isn't bad enough for you, then enjoy dealing with the world's worst/annoying drunk people.\n",
      "text_instance:  Will never, ever go back.\n",
      "text_instance:  The atmosphere here is fun.\n",
      "text_instance:  The pancake was also really good and pretty large at that.\n",
      "text_instance:  All of the tapas dishes were delicious!\n",
      "text_instance:  The chains, which I'm no fan of, beat this place easily.\n",
      "text_instance:  Everyone is very attentive, providing excellent customer service.\n",
      "text_instance:  The staff are also very friendly and efficient.\n",
      "text_instance:  Loved it...friendly servers, great food, wonderful and imaginative menu.\n",
      "text_instance:  After the disappointing dinner we went elsewhere for dessert.\n",
      "text_instance:  Perhaps I caught them on an off night judging by the other reviews, but I'm not inspired to go back.\n",
      "text_instance:  I was disgusted because I was pretty sure that was human hair.\n",
      "text_instance:  The sweet potato fries were very good and seasoned well.\n",
      "text_instance:  This was like the final blow!\n",
      "text_instance:  This place is a jewel in Las Vegas, and exactly what I've been hoping to find in nearly ten years living here.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "text_instance:  She ordered a toasted English muffin that came out untoasted.\n",
      "text_instance:  I ordered Albondigas soup - which was just warm - and tasted like tomato soup with frozen meatballs.\n",
      "text_instance:  The staff is super nice and very quick even with the crazy crowds of the downtown juries, lawyers, and court staff.\n",
      "text_instance:  The only thing I did like was the prime rib and dessert section.\n",
      "text_instance:  I LOVED their mussels cooked in this wine reduction, the duck was tender, and their potato dishes were delicious.\n",
      "text_instance:  Food was good, service was good, Prices were good.\n",
      "text_instance:  Crust is not good.\n",
      "text_instance:  The crêpe was delicate and thin and moist.\n",
      "text_instance:  That said, our mouths and bellies were still quite pleased.\n",
      "text_instance:  An absolute must visit!\n",
      "text_instance:  When I received my Pita it was huge it did have a lot of meat in it so thumbs up there.\n",
      "text_instance:  Service was fantastic.\n",
      "text_instance:  Friend's pasta -- also bad, he barely touched it.\n",
      "text_instance:  The real disappointment was our waiter.\n",
      "text_instance:  My wife had the Lobster Bisque soup which was lukewarm.\n",
      "text_instance:  They brought a fresh batch of fries and I was thinking yay something warm but no!\n",
      "text_instance:  Thus far, have only visited twice and the food was absolutely delicious each time.\n",
      "text_instance:  This wonderful experience made this place a must-stop whenever we are in town again.\n",
      "text_instance:  Appetite instantly gone.\n",
      "text_instance:  We were promptly greeted and seated.\n",
      "text_instance:  Needless to say, we will never be back here again.\n",
      "text_instance:  There is nothing privileged about working/eating there.\n",
      "text_instance:  Bland and flavorless is a good way of describing the barely tepid meat.\n",
      "text_instance:  This place should honestly be blown up.\n",
      "text_instance:  The waiter wasn't helpful or friendly and rarely checked on us.\n",
      "text_instance:  By this point, my friends and I had basically figured out this place was a joke and didn't mind making it publicly and loudly known.\n",
      "text_instance:  I hate those things as much as cheap quality black olives.\n",
      "text_instance:  Both of them were truly unbelievably good, and I am so glad we went back.\n",
      "text_instance:  You get incredibly fresh fish, prepared with care.\n",
      "text_instance:  The restaurant atmosphere was exquisite.\n",
      "text_instance:  Service is quick and friendly.\n",
      "text_instance:  Hot dishes are not hot, cold dishes are close to room temp.I watched staff prepare food with BARE HANDS, no gloves.Everything is deep fried in oil.\n",
      "text_instance:  If you look for authentic Thai food, go else where.\n",
      "text_instance:  You can't beat that.\n",
      "text_instance:  The Veggitarian platter is out of this world!\n",
      "text_instance:  It lacked flavor, seemed undercooked, and dry.\n",
      "text_instance:  All the bread is made in-house!\n",
      "text_instance:  I consider this theft.\n",
      "text_instance:  Maybe if they weren't cold they would have been somewhat edible.\n",
      "text_instance:  My husband and I ate lunch here and were very disappointed with the food and service.\n",
      "text_instance:  The warm beer didn't help.\n",
      "text_instance:  I can assure you that you won't be disappointed.\n",
      "text_instance:  The live music on Fridays totally blows.\n",
      "text_instance:  The seafood was fresh and generous in portion.\n",
      "text_instance:  This place is two thumbs up....way up.\n",
      "text_instance:  The Greek dressing was very creamy and flavorful.\n",
      "text_instance:  I swung in to give them a try but was deeply disappointed.\n",
      "text_instance:  I love the decor with the Chinese calligraphy wall paper.\n",
      "text_instance:  The food was excellent and service was very good.\n",
      "text_instance:  The restaurant is very clean and has a family restaurant feel to it.\n",
      "text_instance:  Much better than the other AYCE sushi place I went to in Vegas.\n",
      "text_instance:  It was so bad, I had lost the heart to finish it.\n",
      "text_instance:  For service, I give them no stars.\n",
      "text_instance:  Def coming back to bowl next time\n",
      "text_instance:  The chips and salsa were really good, the salsa was very fresh.\n",
      "text_instance:  We got sitting fairly fast, but, ended up waiting 40 minutes just to place our order, another 30 minutes before the food arrived.\n",
      "text_instance:  The patio seating was very comfortable.\n",
      "text_instance:  I have been in more than a few bars in Vegas, and do not ever recall being charged for tap water.\n",
      "text_instance:  The food was very good and I enjoyed every mouthful, an enjoyable relaxed venue for couples small family groups etc.\n",
      "text_instance:  I will come back here every time I'm in Vegas.\n",
      "text_instance:  Food was below average.\n",
      "text_instance:  This is my new fav Vegas buffet spot.\n",
      "text_instance:  The sergeant pepper beef sandwich with auju sauce is an excellent sandwich as well.\n",
      "text_instance:  The descriptions said \"yum yum sauce\" and another said \"eel sauce\", yet another said \"spicy mayo\"...well NONE of the rolls had sauces on them.\n",
      "text_instance:  I'd rather eat airline food, seriously.\n",
      "text_instance:  This was my first crawfish experience, and it was delicious!\n",
      "text_instance:  Weird vibe from owners.\n",
      "text_instance:  I am far from a sushi connoisseur but I can definitely tell the difference between good food and bad food and this was certainly bad food.\n",
      "text_instance:  Their daily specials are always a hit with my group.\n",
      "text_instance:  The RI style calamari was a joke.\n",
      "text_instance:  I had the mac salad and it was pretty bland so I will not be getting that again.\n",
      "text_instance:  After waiting an hour and being seated, I was not in the greatest of moods.\n",
      "text_instance:  This isn't a small family restaurant, this is a fine dining establishment.\n",
      "text_instance:  AN HOUR... seriously?\n",
      "text_instance:  Will go back next trip out.\n",
      "text_instance:  Would come back again if I had a sushi craving while in Vegas.\n",
      "text_instance:  This place has a lot of promise but fails to deliver.\n",
      "text_instance:  The nachos are a MUST HAVE!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.38+/-0.041\n",
      "PGU:0.258+/-0.033\n",
      "algo: lime1000\n",
      "text_instance:  the presentation of the food was awful.\n",
      "text_instance:  Worst food/service I've had in a while.\n",
      "text_instance:  Never again will I be dining at this place!\n",
      "text_instance:  I guess maybe we went on an off night but it was disgraceful.\n",
      "text_instance:  As a sushi lover avoid this place by all means.\n",
      "text_instance:  The ambiance isn't much better.\n",
      "text_instance:  This hole in the wall has great Mexican street tacos, and friendly staff.\n",
      "text_instance:  If the food isn't bad enough for you, then enjoy dealing with the world's worst/annoying drunk people.\n",
      "text_instance:  Will never, ever go back.\n",
      "text_instance:  The atmosphere here is fun.\n",
      "text_instance:  The pancake was also really good and pretty large at that.\n",
      "text_instance:  All of the tapas dishes were delicious!\n",
      "text_instance:  The chains, which I'm no fan of, beat this place easily.\n",
      "text_instance:  Everyone is very attentive, providing excellent customer service.\n",
      "text_instance:  The staff are also very friendly and efficient.\n",
      "text_instance:  Loved it...friendly servers, great food, wonderful and imaginative menu.\n",
      "text_instance:  After the disappointing dinner we went elsewhere for dessert.\n",
      "text_instance:  Perhaps I caught them on an off night judging by the other reviews, but I'm not inspired to go back.\n",
      "text_instance:  I was disgusted because I was pretty sure that was human hair.\n",
      "text_instance:  The sweet potato fries were very good and seasoned well.\n",
      "text_instance:  This was like the final blow!\n",
      "text_instance:  This place is a jewel in Las Vegas, and exactly what I've been hoping to find in nearly ten years living here.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "text_instance:  She ordered a toasted English muffin that came out untoasted.\n",
      "text_instance:  I ordered Albondigas soup - which was just warm - and tasted like tomato soup with frozen meatballs.\n",
      "text_instance:  The staff is super nice and very quick even with the crazy crowds of the downtown juries, lawyers, and court staff.\n",
      "text_instance:  The only thing I did like was the prime rib and dessert section.\n",
      "text_instance:  I LOVED their mussels cooked in this wine reduction, the duck was tender, and their potato dishes were delicious.\n",
      "text_instance:  Food was good, service was good, Prices were good.\n",
      "text_instance:  Crust is not good.\n",
      "text_instance:  The crêpe was delicate and thin and moist.\n",
      "text_instance:  That said, our mouths and bellies were still quite pleased.\n",
      "text_instance:  An absolute must visit!\n",
      "text_instance:  When I received my Pita it was huge it did have a lot of meat in it so thumbs up there.\n",
      "text_instance:  Service was fantastic.\n",
      "text_instance:  Friend's pasta -- also bad, he barely touched it.\n",
      "text_instance:  The real disappointment was our waiter.\n",
      "text_instance:  My wife had the Lobster Bisque soup which was lukewarm.\n",
      "text_instance:  They brought a fresh batch of fries and I was thinking yay something warm but no!\n",
      "text_instance:  Thus far, have only visited twice and the food was absolutely delicious each time.\n",
      "text_instance:  This wonderful experience made this place a must-stop whenever we are in town again.\n",
      "text_instance:  Appetite instantly gone.\n",
      "text_instance:  We were promptly greeted and seated.\n",
      "text_instance:  Needless to say, we will never be back here again.\n",
      "text_instance:  There is nothing privileged about working/eating there.\n",
      "text_instance:  Bland and flavorless is a good way of describing the barely tepid meat.\n",
      "text_instance:  This place should honestly be blown up.\n",
      "text_instance:  The waiter wasn't helpful or friendly and rarely checked on us.\n",
      "text_instance:  By this point, my friends and I had basically figured out this place was a joke and didn't mind making it publicly and loudly known.\n",
      "text_instance:  I hate those things as much as cheap quality black olives.\n",
      "text_instance:  Both of them were truly unbelievably good, and I am so glad we went back.\n",
      "text_instance:  You get incredibly fresh fish, prepared with care.\n",
      "text_instance:  The restaurant atmosphere was exquisite.\n",
      "text_instance:  Service is quick and friendly.\n",
      "text_instance:  Hot dishes are not hot, cold dishes are close to room temp.I watched staff prepare food with BARE HANDS, no gloves.Everything is deep fried in oil.\n",
      "text_instance:  If you look for authentic Thai food, go else where.\n",
      "text_instance:  You can't beat that.\n",
      "text_instance:  The Veggitarian platter is out of this world!\n",
      "text_instance:  It lacked flavor, seemed undercooked, and dry.\n",
      "text_instance:  All the bread is made in-house!\n",
      "text_instance:  I consider this theft.\n",
      "text_instance:  Maybe if they weren't cold they would have been somewhat edible.\n",
      "text_instance:  My husband and I ate lunch here and were very disappointed with the food and service.\n",
      "text_instance:  The warm beer didn't help.\n",
      "text_instance:  I can assure you that you won't be disappointed.\n",
      "text_instance:  The live music on Fridays totally blows.\n",
      "text_instance:  The seafood was fresh and generous in portion.\n",
      "text_instance:  This place is two thumbs up....way up.\n",
      "text_instance:  The Greek dressing was very creamy and flavorful.\n",
      "text_instance:  I swung in to give them a try but was deeply disappointed.\n",
      "text_instance:  I love the decor with the Chinese calligraphy wall paper.\n",
      "text_instance:  The food was excellent and service was very good.\n",
      "text_instance:  The restaurant is very clean and has a family restaurant feel to it.\n",
      "text_instance:  Much better than the other AYCE sushi place I went to in Vegas.\n",
      "text_instance:  It was so bad, I had lost the heart to finish it.\n",
      "text_instance:  For service, I give them no stars.\n",
      "text_instance:  Def coming back to bowl next time\n",
      "text_instance:  The chips and salsa were really good, the salsa was very fresh.\n",
      "text_instance:  We got sitting fairly fast, but, ended up waiting 40 minutes just to place our order, another 30 minutes before the food arrived.\n",
      "text_instance:  The patio seating was very comfortable.\n",
      "text_instance:  I have been in more than a few bars in Vegas, and do not ever recall being charged for tap water.\n",
      "text_instance:  The food was very good and I enjoyed every mouthful, an enjoyable relaxed venue for couples small family groups etc.\n",
      "text_instance:  I will come back here every time I'm in Vegas.\n",
      "text_instance:  Food was below average.\n",
      "text_instance:  This is my new fav Vegas buffet spot.\n",
      "text_instance:  The sergeant pepper beef sandwich with auju sauce is an excellent sandwich as well.\n",
      "text_instance:  The descriptions said \"yum yum sauce\" and another said \"eel sauce\", yet another said \"spicy mayo\"...well NONE of the rolls had sauces on them.\n",
      "text_instance:  I'd rather eat airline food, seriously.\n",
      "text_instance:  This was my first crawfish experience, and it was delicious!\n",
      "text_instance:  Weird vibe from owners.\n",
      "text_instance:  I am far from a sushi connoisseur but I can definitely tell the difference between good food and bad food and this was certainly bad food.\n",
      "text_instance:  Their daily specials are always a hit with my group.\n",
      "text_instance:  The RI style calamari was a joke.\n",
      "text_instance:  I had the mac salad and it was pretty bland so I will not be getting that again.\n",
      "text_instance:  After waiting an hour and being seated, I was not in the greatest of moods.\n",
      "text_instance:  This isn't a small family restaurant, this is a fine dining establishment.\n",
      "text_instance:  AN HOUR... seriously?\n",
      "text_instance:  Will go back next trip out.\n",
      "text_instance:  Would come back again if I had a sushi craving while in Vegas.\n",
      "text_instance:  This place has a lot of promise but fails to deliver.\n",
      "text_instance:  The nachos are a MUST HAVE!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.382+/-0.038\n",
      "PGU:0.239+/-0.033\n",
      "algo: random\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.271+/-0.029\n",
      "PGU:0.251+/-0.027\n"
     ]
    }
   ],
   "source": [
    "algos = ['lime16', 'lime1000', 'random'] #'random', 'lime'\n",
    "for algo in algos:\n",
    "    print('algo:', algo)\n",
    "    if algo == 'lime16':\n",
    "        param_dicts['lime']['n_samples'] = 16\n",
    "    elif algo == 'lime1000':\n",
    "        param_dicts['lime']['n_samples'] = 1000\n",
    "        \n",
    "    if algo == 'random':\n",
    "        do_random_baseline = True\n",
    "        PGI_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], np.zeros((num_test_samps*10,1,k)), model, 0, num_test_samps, k, do_pgu=False, do_random_baseline=do_random_baseline)\n",
    "        PGU_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], np.zeros((num_test_samps*10,1,k)), model, 0, num_test_samps, k, do_pgu=True, do_random_baseline=do_random_baseline)\n",
    "    else:  # lime\n",
    "        do_random_baseline = False\n",
    "        \n",
    "        all_exps = []\n",
    "        num_valid_exps = 0\n",
    "        for index, test_sentence in enumerate(X_test_lime):\n",
    "            if num_valid_exps == num_test_samps:\n",
    "                print('num_test_samps reached')\n",
    "                break\n",
    "            if len(tokenizer(test_sentence)) <= k:\n",
    "                print(f'Less than {k} tokens in sentence. Skipping the sentence:', test_sentence)\n",
    "                all_exps.append([])\n",
    "                continue\n",
    "                \n",
    "            explainer = Explainer(method='lime', model=classifier_fn, dataset_tensor=X_train_lime, param_dict=param_dicts['lime'])\n",
    "            explanations = explainer.get_explanation([test_sentence], seed=0, disable_tqdm=True)\n",
    "            \n",
    "            # list of tuples: (word, importance), sorted by importance. Get top-k words.\n",
    "            exps = [[exp.as_list()[i][0] for i in range(len(exp.as_list()[:k]))] for exp in explanations] \n",
    "            all_exps.append(exps)\n",
    "            num_valid_exps += 1\n",
    "        PGI_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], all_exps, model, 0, num_test_samps, k, do_pgu=False, do_random_baseline=do_random_baseline)\n",
    "        PGU_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], all_exps, model, 0, num_test_samps, k, do_pgu=True, do_random_baseline=do_random_baseline)\n",
    "\n",
    "        \n",
    "    print('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)))\n",
    "    print('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)))\n",
    "    \n",
    "    # save out to file\n",
    "    with open(f'outputs/TextFaithfulnessResults/{data_name}/faithfulness_{experiment_id}_{algo}_{data_name}_{model_name}_AUC-k{k}.txt', 'w') as f:\n",
    "        f.write('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)) + '\\n')\n",
    "        f.write('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T03:46:12.121158Z",
     "start_time": "2024-06-14T03:46:04.058324Z"
    }
   },
   "id": "c488da86435d7f7d",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, LayerGradientShap, LayerDeepLift, LayerGradientXActivation, LayerActivation, LayerConductance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:44.425275Z",
     "start_time": "2024-06-13T08:05:44.423337Z"
    }
   },
   "id": "cc150a515ef354d1",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# lig is LayerIntegratedGradients\n",
    "# lgshap is LayerGradientShap\n",
    "# ldl is LayerDeepLift\n",
    "# lgxa is LayerGradientXActivation\n",
    "# la is LayerActivation\n",
    "# lc is LayerConductance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:45.147824Z",
     "start_time": "2024-06-13T08:05:45.145638Z"
    }
   },
   "id": "f4442fb8983246a9",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo lig\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.291+/-0.04\n",
      "PGU:0.346+/-0.04\n",
      "algo lgshap\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.291+/-0.039\n",
      "PGU:0.355+/-0.038\n",
      "algo ldl\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.301+/-0.038\n",
      "PGU:0.305+/-0.038\n",
      "algo lgxa\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.28+/-0.038\n",
      "PGU:0.331+/-0.042\n",
      "algo la\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.212+/-0.035\n",
      "PGU:0.371+/-0.041\n",
      "algo lc\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "num_test_samps reached\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Fantastic food!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Crust is not good.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: An absolute must visit!\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Service was fantastic.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Appetite instantly gone.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: I consider this theft.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Food was below average.\n",
      "Less than 3 tokens in sentence. Skipping the sentence: Weird vibe from owners.\n",
      "PGI:0.17+/-0.03\n",
      "PGU:0.355+/-0.042\n"
     ]
    }
   ],
   "source": [
    "grad_algos = ['lig', 'lgshap', 'ldl', 'lgxa', 'la', 'lc']\n",
    "for algo in grad_algos:\n",
    "    print('algo', algo)\n",
    "    all_exps = []\n",
    "    num_valid_exps = 0\n",
    "    if algo == 'lig':\n",
    "        attr_method = LayerIntegratedGradients(model, model.embeddings.embedding)\n",
    "    elif algo == 'lgshap':\n",
    "        attr_method = LayerGradientShap(model, model.embeddings.embedding)\n",
    "    elif algo == 'ldl':\n",
    "        attr_method = LayerDeepLift(model, model.embeddings.embedding)\n",
    "    elif algo == 'lgxa':\n",
    "        attr_method = LayerGradientXActivation(model, model.embeddings.embedding)\n",
    "    elif algo == 'la':\n",
    "        attr_method = LayerActivation(model, model.embeddings.embedding)\n",
    "    elif algo == 'lc':\n",
    "        attr_method = LayerConductance(model, model.embeddings.embedding)\n",
    "    else:\n",
    "        print('algo not implemented yet')\n",
    "        continue\n",
    "    model = model.to('cpu')\n",
    "    for index, test_sentence in enumerate(X_test_lime):\n",
    "        if num_valid_exps == num_test_samps:\n",
    "            print('num_test_samps reached')\n",
    "            break\n",
    "        if len(tokenizer(test_sentence)) <= k:\n",
    "            print(f'Less than {k} tokens in sentence. Skipping the sentence:', test_sentence)\n",
    "            all_exps.append([])\n",
    "            continue\n",
    "        # print('test_sentence:', test_sentence)\n",
    "        tokenized_input = torch.tensor([voc[t] for t in tokenizer(test_sentence)])\n",
    "        \n",
    "        ref = torch.tensor([[0]*len(tokenized_input)], dtype=torch.long)\n",
    "        \n",
    "        x = tokenized_input.unsqueeze(0)\n",
    "        \n",
    "        if algo == 'lig':\n",
    "            attributions, delta = attr_method.attribute(x.float(), ref, n_steps=500, return_convergence_delta=True, target=0)\n",
    "        elif algo == 'lgxa':\n",
    "            attributions = attr_method.attribute(x.float(), target=0)\n",
    "        elif algo == 'la':\n",
    "            attributions = attr_method.attribute(x.float())\n",
    "        else:\n",
    "            attributions, delta = attr_method.attribute(x.float(), ref, return_convergence_delta=True, target=0)\n",
    "        attributions = attributions.sum(dim=2).squeeze(0)\n",
    "        attributions = attributions / torch.norm(attributions)\n",
    "        \n",
    "        # get indices of topk attributions\n",
    "        topk_inds = torch.argsort(attributions, descending=True)[:k]\n",
    "        top_k_voc_inds = tokenized_input[topk_inds]\n",
    "        top_k_words = [voc.itos[ind] for ind in top_k_voc_inds]\n",
    "        all_exps.append([top_k_words])\n",
    "        num_valid_exps += 1\n",
    "    \n",
    "    PGI_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], all_exps, model, 0, num_test_samps, k, do_pgu=False, do_random_baseline=False)\n",
    "    PGU_AUC = calculateFaithfulnessAUC_text(X_test_lime[:num_test_samps], all_exps, model, 0, num_test_samps, k, do_pgu=True, do_random_baseline=False)\n",
    "    print('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)))\n",
    "    print('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)))\n",
    "    \n",
    "    # save out to file\n",
    "    with open(f'outputs/TextFaithfulnessResults/{data_name}/faithfulness_{experiment_id}_{algo}_{data_name}_{model_name}_AUC-k{k}.txt', 'w') as f:\n",
    "        f.write('PGI:' + str(round(np.mean(PGI_AUC), 3)) + '+/-' + str(round(np.std(PGI_AUC)/np.sqrt(len(PGI_AUC)), 3)) + '\\n')\n",
    "        f.write('PGU:' + str(round(np.mean(PGU_AUC), 3)) + '+/-' + str(round(np.std(PGU_AUC)/np.sqrt(len(PGU_AUC)), 3)) + '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T08:05:51.564654Z",
     "start_time": "2024-06-13T08:05:46.698086Z"
    }
   },
   "id": "df880ff0786a7e53",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T04:03:11.365461Z",
     "start_time": "2024-06-13T04:03:11.363923Z"
    }
   },
   "id": "81ffe4b3ee714d93",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc78449be67fb04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# all_labels_train = []\n",
    "# all_tokenized_inputs_w_offsets_train = []\n",
    "# # all_offsets_train = []\n",
    "# # all_tokenized_inputs_no_offsets_train = []\n",
    "# for index, input_tuple in enumerate(loader_train):\n",
    "#     (labels, inputs) = input_tuple\n",
    "#     all_labels_train.append(labels[0])\n",
    "#     all_tokenized_inputs_w_offsets_train.append(inputs[0])\n",
    "#     # all_offsets_train.append(offsets)\n",
    "#     # all_tokenized_inputs_no_offsets_train.append(tokenized_list[0])\n",
    "#     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:39.703028Z",
     "start_time": "2024-06-12T06:31:39.701060Z"
    }
   },
   "id": "b0ca9c68821490a7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# \n",
    "# padded_inputs_train = pad_sequence(all_tokenized_inputs_w_offsets_train, batch_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:39.918758Z",
     "start_time": "2024-06-12T06:31:39.917032Z"
    }
   },
   "id": "80fc20bde2bf9033",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Generate embeddings\n",
    "# def generate_embeddings(texts):\n",
    "#     embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     batch_size = 256  # Adjust based on your system's memory capacity\n",
    "#     embeddings = []\n",
    "#     # print('transforming data...')\n",
    "#     num_batches = len(texts) // batch_size\n",
    "#     for i in range(0, len(texts), batch_size):\n",
    "#         # if i % 100 == 0:\n",
    "#             # print(f'Processing batch {i // batch_size + 1}/{num_batches}')\n",
    "#         batch = texts[i:i + batch_size]\n",
    "#         batch_embeddings = embedding_model.encode(batch)\n",
    "#         embeddings.extend(batch_embeddings)\n",
    "#     return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:42.495470Z",
     "start_time": "2024-06-12T06:31:42.493549Z"
    }
   },
   "id": "7b255378859a06e8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def classifier_fn(sentences):\n",
    "#     embeddings = generate_embeddings(sentences)\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(torch.Tensor(embeddings))\n",
    "#     # return the class argmax\n",
    "#     # preds = torch.argmax(preds, dim=1)\n",
    "#     return preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:43.189967Z",
     "start_time": "2024-06-12T06:31:43.187665Z"
    }
   },
   "id": "5f113f8a041fe88f",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# padded_inputs_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:40.091735Z",
     "start_time": "2024-06-12T06:31:40.090109Z"
    }
   },
   "id": "10d245f8e776b23c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# all_labels_test = []\n",
    "# all_tokenized_inputs_w_offsets_test = []\n",
    "# # all_offsets_test = []\n",
    "# # all_tokenized_inputs_no_offsets_test = []\n",
    "# for index, input_tuple in enumerate(loader_test):\n",
    "#     (labels, inputs) = input_tuple\n",
    "#     all_labels_test.append(labels[0])\n",
    "#     all_tokenized_inputs_w_offsets_test.append(inputs[0])\n",
    "#     # all_offsets_test.append(offsets)\n",
    "#     # all_tokenized_inputs_no_offsets_test.append(tokenized_list[0])\n",
    "# padded_inputs_test = pad_sequence(all_tokenized_inputs_w_offsets_test, batch_first=True)\n",
    "# padded_inputs_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T06:31:40.566040Z",
     "start_time": "2024-06-12T06:31:40.564156Z"
    }
   },
   "id": "4cb3d43cd7f4704c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2afa9eb75927e42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# #get the top k indices given a list of strings\n",
    "# def get_top_k_indices(top_k, feature_names):\n",
    "#     top_k_indices = []\n",
    "#     for feature in top_k:\n",
    "#         top_k_indices.append(feature_names.index(feature))\n",
    "#     return top_k_indices\n",
    "# \n",
    "# # top_k_inds_0 = get_top_k_indices(LIME_exps[0], X_test_sentences[0].split()) # [10, 34, 12]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-04T19:54:22.172931Z",
     "start_time": "2024-06-04T19:54:22.170422Z"
    }
   },
   "id": "82f2408a422b0c54",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['commodity',\n 'salutary',\n 'thoroughly',\n 'undecomposed',\n 'honorable',\n 'safe',\n 'unspoiled',\n 'skilful',\n 'proficient',\n 'dependable',\n 'practiced',\n 'respectable',\n 'well',\n 'near',\n 'full',\n 'goodness',\n 'soundly',\n 'secure',\n 'just',\n 'serious',\n 'beneficial',\n 'sound',\n 'in_effect',\n 'upright',\n 'right',\n 'estimable',\n 'in_force',\n 'honest',\n 'trade_good',\n 'dear',\n 'effective',\n 'good',\n 'expert',\n 'ripe',\n 'adept',\n 'unspoilt',\n 'skillful']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "# \n",
    "# def get_synonyms(word):\n",
    "#     synonyms = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             synonyms.add(lemma.name())  # add the synonyms\n",
    "#     return list(synonyms)\n",
    "# \n",
    "# def get_antonyms(word):\n",
    "#     antonyms = set()\n",
    "#     for syn in wordnet.synsets(word):\n",
    "#         for lemma in syn.lemmas():\n",
    "#             if lemma.antonyms():  # Check if antonyms are available\n",
    "#                 antonyms.add(lemma.antonyms()[0].name())  # add the antonyms\n",
    "#     return list(antonyms)\n",
    "# \n",
    "# get_synonyms('good')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-05T23:51:30.343676Z",
     "start_time": "2024-06-05T23:51:28.367097Z"
    }
   },
   "id": "f6ca9513fe88afc2",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence ['I', 'bought', 'this', 'for', 'my', 'wife', 'She', 'says', 'that', 'it', 'makes', 'the', 'process', 'easier', 'and', 'much', 'faster', 'I', 'say', 'that', 'the', 'results', 'are', 'great', 'a', 'very', 'noticeable', 'difference', 'My', 'daughter', 'says', 'shes', 'wants', 'one', 'too']\n",
      "makes\n",
      "index 10\n",
      "synonym piddle\n",
      "['I', 'bought', 'this', 'for', 'my', 'wife', 'She', 'says', 'that', 'it', 'piddle', 'the', 'process', 'easier', 'and', 'much', 'faster', 'I', 'say', 'that', 'the', 'results', 'are', 'great', 'a', 'very', 'noticeable', 'difference', 'My', 'daughter', 'says', 'shes', 'wants', 'one', 'too']\n",
      "too\n",
      "index 34\n",
      "synonym excessively\n",
      "['I', 'bought', 'this', 'for', 'my', 'wife', 'She', 'says', 'that', 'it', 'piddle', 'the', 'process', 'easier', 'and', 'much', 'faster', 'I', 'say', 'that', 'the', 'results', 'are', 'great', 'a', 'very', 'noticeable', 'difference', 'My', 'daughter', 'says', 'shes', 'wants', 'one', 'excessively']\n",
      "process\n",
      "index 12\n",
      "synonym sue\n",
      "['I', 'bought', 'this', 'for', 'my', 'wife', 'She', 'says', 'that', 'it', 'piddle', 'the', 'sue', 'easier', 'and', 'much', 'faster', 'I', 'say', 'that', 'the', 'results', 'are', 'great', 'a', 'very', 'noticeable', 'difference', 'My', 'daughter', 'says', 'shes', 'wants', 'one', 'excessively']\n"
     ]
    }
   ],
   "source": [
    "# # get the words at those indices\n",
    "# X_test_sentences_temp = X_test_sentences[0].split().copy()\n",
    "# print('original sentence', X_test_sentences_temp)\n",
    "# for idx, top_k_word in enumerate(LIME_exps[0]):\n",
    "#     print(top_k_word)\n",
    "#     # print(get_synonyms(top_k_word))\n",
    "#     print('index', top_k_inds_0[idx])\n",
    "#     synonym = get_synonyms(top_k_word)[0]\n",
    "#     X_test_sentences_temp[top_k_inds_0[idx]] = synonym\n",
    "#     print('synonym', synonym)\n",
    "#     print(X_test_sentences_temp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T00:04:23.890099Z",
     "start_time": "2024-06-06T00:04:23.887165Z"
    }
   },
   "id": "97699c455a9801be",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#note: it’s actually not possible to do perturbations in the embedding space. I am asking the LLM (and post hoc explainers) for the most important WORDS for classification. This is only a problem because I’m generating SENTENCE level embeddings and training the model on that. I don’t see a way to generate perturbations in the embedding space because what would I be perturbing?\n",
    "\n",
    "# # Function to perturb the important features\n",
    "# def perturb_features(embedding, top_k_indices, std_dev=0.1):\n",
    "#     perturbed_embedding = embedding.clone()\n",
    "#     perturbed_embedding[0, top_k_indices] += torch.randn(top_k_indices.size(0)) * std_dev\n",
    "#     return perturbed_embedding\n",
    "# perturb_features(torch.tensor(X_test[0]), torch.tensor(top_k_inds_0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T00:08:23.407457Z",
     "start_time": "2024-06-06T00:08:23.403929Z"
    }
   },
   "id": "8398eda5702fe210",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import torch\n",
    "# class WrapperModel(torch.nn.Module):\n",
    "#     def __init__(self, original_model):\n",
    "#         super().__init__()\n",
    "#         self.original_model = original_model\n",
    "# \n",
    "#     def forward(self, texts):\n",
    "#         print('texts', texts)\n",
    "#         # Assume preprocessing and tokenization here\n",
    "#         # processed_inputs = [torch.tensor([voc[t] for t in tokens]) for tokens in texts]\n",
    "#         # inputs = torch.cat(processed_inputs)\n",
    "#         offsets = torch.tensor([0])\n",
    "#         print('offsets', offsets)\n",
    "#         \n",
    "#         # Ensure the model is in the right mode (e.g., eval)\n",
    "#         self.original_model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             predictions = self.original_model(texts, offsets)\n",
    "#         print('predictions', predictions)\n",
    "#         return predictions\n",
    "# \n",
    "# # Usage\n",
    "# wrapper_model = WrapperModel(model)\n",
    "# # Now you can use wrapper_model to make predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf5921186fa67c70",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def classifier_fn(inputs): # TODO finish this...... rework this for LIME\n",
    "#     print('texts',inputs)\n",
    "#     # Assuming `tokenizer` and `vocab` are defined elsewhere and accessible here\n",
    "#     # tokenized_texts = [tokenizer(text) for text in texts]\n",
    "#     # processed_inputs = [torch.tensor([voc.stoi[token] for token in text if token in voc.stoi]) for text in texts]\n",
    "#     # processed_inputs = [torch.tensor([voc[t] for t in tokens]) for tokens in texts]\n",
    "#     # offsets = [0] + [len(text) for text in texts[:-1]]\n",
    "#     # offsets = torch.tensor(offsets).cumsum(dim=0)\n",
    "# \n",
    "#     # assuming we're passing in 1 tokenized sentence at a time!\n",
    "#     offsets = torch.tensor([0])\n",
    "#     # inputs = torch.cat(processed_inputs)\n",
    "#     # offsets = torch.tensor(offsets)\n",
    "#     print('inputs',inputs)\n",
    "#     print('offsets',offsets)\n",
    "#     # Assuming the model is already on the correct device and in evaluation mode\n",
    "#     # model.eval()\n",
    "#     # with torch.no_grad():\n",
    "#     predictions = model(inputs, offsets)\n",
    "# \n",
    "#     print('predictions',predictions)\n",
    "#     return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T04:16:01.111669Z",
     "start_time": "2024-06-12T04:16:01.109537Z"
    }
   },
   "id": "e2b9a5613019a1b9",
   "execution_count": 247
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from captum.attr import Lime, LimeBase\n",
    "# from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n",
    "# from torch.nn import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T04:16:01.895836Z",
     "start_time": "2024-06-12T04:16:01.893692Z"
    }
   },
   "id": "a96c05eb3c4e7196",
   "execution_count": 248
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # remove the batch dimension for the embedding-bag model\n",
    "# def forward_func(text):\n",
    "#     print('text', text)\n",
    "#     print('text.shape', text.shape)\n",
    "#     return model(text)\n",
    "# \n",
    "# # encode text indices into latent representations & calculate cosine similarity\n",
    "# def exp_embedding_cosine_distance(original_inp, perturbed_inp, _, **kwargs):\n",
    "#     print('Making embeddings!')\n",
    "#     print('original_inp', original_inp)\n",
    "#     print('perturbed_inp', perturbed_inp)\n",
    "#     original_emb = model.embeddings.embedding(original_inp.unsqueeze(0))\n",
    "#     perturbed_emb = model.embeddings.embedding(perturbed_inp)\n",
    "#     print('embedding shapes:')\n",
    "#     print('original_emb.shape', original_emb.shape)\n",
    "#     print('perturbed_emb.shape', perturbed_emb.shape)\n",
    "#     distance = 1 - F.cosine_similarity(original_emb.squeeze(), perturbed_emb.squeeze(), dim=0)\n",
    "#     print('distance:', distance)\n",
    "#     return torch.exp(-1 * (distance ** 2) / 2)\n",
    "# \n",
    "# # binary vector where each word is selected independently and uniformly at random\n",
    "# def bernoulli_perturb(text, **kwargs):\n",
    "#     probs = torch.ones_like(text) * 0.5\n",
    "#     return torch.bernoulli(probs).long()\n",
    "# \n",
    "# # remove absenst token based on the intepretable representation sample\n",
    "# def interp_to_input(interp_sample, original_input, **kwargs):\n",
    "#     # temp = original_input[interp_sample.bool()].view(original_input.size(0), -1)\n",
    "#     temp = original_input[interp_sample.bool()].view(1, -1)\n",
    "# \n",
    "#     print(\"interp_sample\",interp_sample)\n",
    "#     print('temp',temp)\n",
    "#     print('original_input',original_input)\n",
    "#     print(interp_sample.shape, temp.shape)\n",
    "#     len_original = len(original_input.squeeze())\n",
    "#     len_temp = temp.squeeze().shape[0]\n",
    "#     # pad with 0s temp.shape torch.Size([1, 5]) to torch.Size([1, 10])\n",
    "#     if len_original > len_temp:\n",
    "#         temp = F.pad(temp, (0, len_original - len_temp), 'constant', 0)\n",
    "#     \n",
    "#     print('temp',temp)\n",
    "#     print('temp.unsqueeze(0).shape', temp.unsqueeze(0).shape)\n",
    "#     return temp.unsqueeze(0)\n",
    "# \n",
    "# lasso_lime_base = LimeBase(\n",
    "#     forward_func, \n",
    "#     interpretable_model=SkLearnLasso(alpha=0.08),\n",
    "#     similarity_func=exp_embedding_cosine_distance,\n",
    "#     perturb_func=bernoulli_perturb,\n",
    "#     perturb_interpretable_space=True,\n",
    "#     from_interp_rep_transform=interp_to_input,\n",
    "#     to_interp_rep_transform=None\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:10:53.438496Z",
     "start_time": "2024-06-12T03:10:53.433268Z"
    }
   },
   "id": "c0e75cb30efae6cb",
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def collate_batch(batch):\n",
    "#     labels = torch.tensor([label for _, label in batch]) \n",
    "#     text_list = [tokenizer(line) for line, _ in batch]\n",
    "#     \n",
    "#     # flatten tokens across the whole batch\n",
    "#     text = torch.tensor([voc[t] for tokens in text_list for t in tokens])\n",
    "#     tokenized_list = [torch.tensor([voc[t] for t in tokens]) for tokens in text_list]\n",
    "#     # the offset of each example\n",
    "#     offsets = torch.tensor(\n",
    "#         [0] + [len(tokens) for tokens in text_list][:-1]\n",
    "#     ).cumsum(dim=0)\n",
    "# \n",
    "#     return labels, text, offsets, tokenized_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:10:53.621038Z",
     "start_time": "2024-06-12T03:10:53.618891Z"
    }
   },
   "id": "5f21df52941e56c7",
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def collate_batch(batch):\n",
    "#     print(batch)\n",
    "#     labels = torch.tensor([label for _, label in batch])\n",
    "#     text_list = [tokenizer(line[0]) for line, _ in batch]\n",
    "#     #pad the text_list to have the same length\n",
    "#     max_len = max([len(tokens) for tokens in text_list])\n",
    "#     for tokens in text_list:\n",
    "#         while len(tokens) < max_len:\n",
    "#             tokens.append('<pad>')\n",
    "# \n",
    "#     inputs = torch.stack([torch.tensor([voc[t] for t in tokens]) for tokens in text_list])\n",
    "# \n",
    "#     return labels, inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:10:53.736188Z",
     "start_time": "2024-06-12T03:10:53.733168Z"
    }
   },
   "id": "62db4fcd56489041",
   "execution_count": 162
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['I think this is a bad product, but needs work'], 1)]\n",
      "tensor([1]) tensor([[   4,  256,    8,    9,   10,  100,   31,    7,   40, 1213,   45]])\n",
      "Prediction probability: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test_label = 1 \n",
    "# test_line = [('I think this is a bad product, but needs work')]\n",
    "# \n",
    "# test_labels, test_text = collate_batch([(test_line, test_label)])\n",
    "# print(test_labels, test_text)\n",
    "# probs = model(test_text).squeeze()\n",
    "# print('Prediction probability:', round(probs[test_labels[0]].item(), 4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:11:19.832853Z",
     "start_time": "2024-06-12T03:11:19.827076Z"
    }
   },
   "id": "6165f8bc55e7bcb",
   "execution_count": 166
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Lime Base attribution:   0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "199dcf9e0ea342c9a28d37ee27ec22e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interp_sample tensor([[[0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1]]])\n",
      "temp tensor([[  10,  100,   40, 1213,   45]])\n",
      "original_input tensor([[[   4,  256,    8,    9,   10,  100,   31,    7,   40, 1213,   45]]])\n",
      "torch.Size([1, 1, 11]) torch.Size([1, 5])\n",
      "temp tensor([[  10,  100,   40, 1213,   45,    0,    0,    0,    0,    0,    0]])\n",
      "temp.unsqueeze(0).shape torch.Size([1, 1, 11])\n",
      "Making embeddings!\n",
      "original_inp tensor([[[   4,  256,    8,    9,   10,  100,   31,    7,   40, 1213,   45]]])\n",
      "perturbed_inp tensor([[[  10,  100,   40, 1213,   45,    0,    0,    0,    0,    0,    0]]])\n",
      "embedding shapes:\n",
      "original_emb.shape torch.Size([1, 1, 1, 11, 8])\n",
      "perturbed_emb.shape torch.Size([1, 1, 11, 8])\n",
      "distance: tensor([0.7507, 0.6202, 0.9325, 0.8418, 1.6475, 0.8301, 1.2224, 1.4550])\n",
      "text tensor([[[  10,  100,   40, 1213,   45,    0,    0,    0,    0,    0,    0]]])\n",
      "text.shape torch.Size([1, 1, 11])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "query should be unbatched 2D or batched 3D tensor but received 4-D query tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[168], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m attrs \u001B[38;5;241m=\u001B[39m \u001B[43mlasso_lime_base\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattribute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_text\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# add batch dimension for Captum\u001B[39;49;00m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# additional_forward_args=(test_offsets,),\u001B[39;49;00m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshow_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAttribution range:\u001B[39m\u001B[38;5;124m'\u001B[39m, attrs\u001B[38;5;241m.\u001B[39mmin()\u001B[38;5;241m.\u001B[39mitem(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto\u001B[39m\u001B[38;5;124m'\u001B[39m, attrs\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/captum/log/__init__.py:42\u001B[0m, in \u001B[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/captum/attr/_core/lime.py:490\u001B[0m, in \u001B[0;36mLimeBase.attribute\u001B[0;34m(self, inputs, target, additional_forward_args, n_samples, perturbations_per_eval, show_progress, **kwargs)\u001B[0m\n\u001B[1;32m    487\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m expanded_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    488\u001B[0m     expanded_target \u001B[38;5;241m=\u001B[39m _expand_target(target, \u001B[38;5;28mlen\u001B[39m(curr_model_inputs))\n\u001B[0;32m--> 490\u001B[0m model_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluate_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    491\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurr_model_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    492\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexpanded_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexpanded_additional_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m show_progress:\n\u001B[1;32m    498\u001B[0m     attr_progress\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/captum/attr/_core/lime.py:549\u001B[0m, in \u001B[0;36mLimeBase._evaluate_batch\u001B[0;34m(self, curr_model_inputs, expanded_target, expanded_additional_args, device)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_evaluate_batch\u001B[39m(\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    544\u001B[0m     curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    547\u001B[0m     device: torch\u001B[38;5;241m.\u001B[39mdevice,\n\u001B[1;32m    548\u001B[0m ):\n\u001B[0;32m--> 549\u001B[0m     model_out \u001B[38;5;241m=\u001B[39m \u001B[43m_run_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_reduce_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurr_model_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpanded_target\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpanded_additional_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model_out, Tensor):\n\u001B[1;32m    556\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m model_out\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(curr_model_inputs), (\n\u001B[1;32m    557\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of outputs is not appropriate, must return \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mone output per perturbed input\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m         )\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/captum/_utils/common.py:531\u001B[0m, in \u001B[0;36m_run_forward\u001B[0;34m(forward_func, inputs, target, additional_forward_args)\u001B[0m\n\u001B[1;32m    528\u001B[0m inputs \u001B[38;5;241m=\u001B[39m _format_inputs(inputs)\n\u001B[1;32m    529\u001B[0m additional_forward_args \u001B[38;5;241m=\u001B[39m _format_additional_forward_args(additional_forward_args)\n\u001B[0;32m--> 531\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mforward_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43madditional_forward_args\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\n\u001B[1;32m    534\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _select_targets(output, target)\n",
      "Cell \u001B[0;32mIn[160], line 5\u001B[0m, in \u001B[0;36mforward_func\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext.shape\u001B[39m\u001B[38;5;124m'\u001B[39m, text\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Harvard/LLM_PostHocExplainer/LLM_Explainer/openxai/ML_Models/ANN/Text_MLP.py:160\u001B[0m, in \u001B[0;36mText_MLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 160\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    161\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# softmax\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Harvard/LLM_PostHocExplainer/LLM_Explainer/openxai/ML_Models/ANN/Text_MLP.py:153\u001B[0m, in \u001B[0;36mText_MLP.encode\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    150\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(x)\n\u001B[1;32m    151\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_encoder(x)\n\u001B[0;32m--> 153\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m x \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/transformer.py:415\u001B[0m, in \u001B[0;36mTransformerEncoder.forward\u001B[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    412\u001B[0m is_causal \u001B[38;5;241m=\u001B[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001B[1;32m    414\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 415\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msrc_key_padding_mask_for_layers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n\u001B[1;32m    418\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_padded_tensor(\u001B[38;5;241m0.\u001B[39m, src\u001B[38;5;241m.\u001B[39msize())\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/transformer.py:749\u001B[0m, in \u001B[0;36mTransformerEncoderLayer.forward\u001B[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    747\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x))\n\u001B[1;32m    748\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 749\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sa_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    750\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[1;32m    752\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/transformer.py:757\u001B[0m, in \u001B[0;36mTransformerEncoderLayer._sa_block\u001B[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sa_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor,\n\u001B[1;32m    756\u001B[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 757\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    758\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    759\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    760\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    761\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/modules/activation.py:1266\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1252\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[1;32m   1253\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[1;32m   1254\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1263\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[1;32m   1264\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1267\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1268\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1269\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1275\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1276\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m   1278\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/functional.py:5291\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   5260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tens_ops):\n\u001B[1;32m   5261\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   5262\u001B[0m         multi_head_attention_forward,\n\u001B[1;32m   5263\u001B[0m         tens_ops,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5288\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[1;32m   5289\u001B[0m     )\n\u001B[0;32m-> 5291\u001B[0m is_batched \u001B[38;5;241m=\u001B[39m \u001B[43m_mha_shape_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5293\u001B[0m \u001B[38;5;66;03m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001B[39;00m\n\u001B[1;32m   5294\u001B[0m \u001B[38;5;66;03m# is batched, run the computation and before returning squeeze the\u001B[39;00m\n\u001B[1;32m   5295\u001B[0m \u001B[38;5;66;03m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001B[39;00m\n\u001B[1;32m   5296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[1;32m   5297\u001B[0m     \u001B[38;5;66;03m# unsqueeze if the input is unbatched\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/LLM_Explainer/lib/python3.9/site-packages/torch/nn/functional.py:5115\u001B[0m, in \u001B[0;36m_mha_shape_check\u001B[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001B[0m\n\u001B[1;32m   5112\u001B[0m             \u001B[38;5;28;01massert\u001B[39;00m attn_mask\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m expected_shape, \\\n\u001B[1;32m   5113\u001B[0m                 (\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `attn_mask` shape to be \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattn_mask\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   5114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 5115\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m   5116\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery should be unbatched 2D or batched 3D tensor but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;241m.\u001B[39mdim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-D query tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   5118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m is_batched\n",
      "\u001B[0;31mAssertionError\u001B[0m: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor"
     ]
    }
   ],
   "source": [
    "# attrs = lasso_lime_base.attribute(\n",
    "#     test_text.unsqueeze(0), # add batch dimension for Captum\n",
    "#     target=test_labels,\n",
    "#     # additional_forward_args=(test_offsets,),\n",
    "#     n_samples=16,\n",
    "#     show_progress=True\n",
    "# ).squeeze(0)\n",
    "# \n",
    "# print('Attribution range:', attrs.min().item(), 'to', attrs.max().item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:11:28.933369Z",
     "start_time": "2024-06-12T03:11:28.782345Z"
    }
   },
   "id": "a40222e472e6807c",
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T02:48:19.163276Z",
     "start_time": "2024-06-12T02:48:19.161619Z"
    }
   },
   "id": "54802e25ad81bb07",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c004fecd3bebd2f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['I love this product, it is amazing'], 1)]\n",
      "Prediction probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "# test_label = 1  # {1: World, 2: Sports, 3: Business, 4: Sci/Tec}\n",
    "# test_line = [('I love this product, it is amazing')]\n",
    "# \n",
    "# test_labels, test_text = collate_batch([(test_line, test_label)])\n",
    "# \n",
    "# probs = model(test_text).squeeze(0)\n",
    "# print('Prediction probability:', round(probs[test_labels[0]].item(), 4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T03:14:23.100055Z",
     "start_time": "2024-06-12T03:14:23.095710Z"
    }
   },
   "id": "1b5ab78207deb708",
   "execution_count": 175
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # remove the batch dimension for the embedding-bag model\n",
    "# def forward_func(text):\n",
    "#     return model(text.squeeze(0))\n",
    "# \n",
    "# # encode text indices into latent representations & calculate cosine similarity\n",
    "# def exp_embedding_cosine_distance(original_inp, perturbed_inp, _, **kwargs):\n",
    "#     original_emb = model.embeddings.embedding(original_inp)\n",
    "#     perturbed_emb = model.embeddings.embedding(perturbed_inp)\n",
    "#     distance = 1 - F.cosine_similarity(original_emb, perturbed_emb, dim=1)\n",
    "#     return torch.exp(-1 * (distance ** 2) / 2)\n",
    "# \n",
    "# # binary vector where each word is selected independently and uniformly at random\n",
    "# def bernoulli_perturb(text, **kwargs):\n",
    "#     probs = torch.ones_like(text) * 0.5\n",
    "#     return torch.bernoulli(probs).long()\n",
    "# \n",
    "# # remove absenst token based on the intepretable representation sample\n",
    "# def interp_to_input(interp_sample, original_input, **kwargs):\n",
    "#     return original_input[interp_sample.bool()].view(original_input.size(0), -1)\n",
    "# \n",
    "# lasso_lime_base = LimeBase(\n",
    "#     forward_func, \n",
    "#     interpretable_model=SkLearnLasso(alpha=0.08),\n",
    "#     similarity_func=exp_embedding_cosine_distance,\n",
    "#     perturb_func=bernoulli_perturb,\n",
    "#     perturb_interpretable_space=True,\n",
    "#     from_interp_rep_transform=interp_to_input,\n",
    "#     to_interp_rep_transform=None\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T02:45:34.873215Z",
     "start_time": "2024-06-12T02:45:34.869433Z"
    }
   },
   "id": "d94426286082dfbb",
   "execution_count": 76
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm_explainer",
   "language": "python",
   "display_name": "LLM_Explainer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
