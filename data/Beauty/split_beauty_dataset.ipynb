{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:29:22.926811Z",
     "start_time": "2024-03-27T22:29:21.614651Z"
    }
   },
   "id": "2f9e2ad29b1eedff",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9250d84e33de9832",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:47.888555Z",
     "start_time": "2024-03-27T22:49:47.885775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the All_Beauty reviews file you downloaded\n",
    "reviews_file_path = 'All_Beauty.jsonl'\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the reviews and the ratings\n",
    "texts, ratings = [], []\n",
    "with open(reviews_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        review = json.loads(line.strip())\n",
    "        texts.append(review['text'])\n",
    "        ratings.append(review['rating'])"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:50.299083Z",
     "start_time": "2024-03-27T22:49:48.160575Z"
    }
   },
   "id": "initial_id",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# get indices where number of words is > 5 and number of words is < 100\n",
    "indices = [i for i, text in enumerate(texts) if 5 < len(text.split()) < 100]\n",
    "texts = [texts[i] for i in indices]\n",
    "ratings = [ratings[i] for i in indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:51.140989Z",
     "start_time": "2024-03-27T22:49:50.300227Z"
    }
   },
   "id": "92298968a0dd52c2",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Convert ratings to binary sentiment (1 for positive, 0 for negative)\n",
    "sentiments = [1 if rating >= 4 else 0 for rating in ratings]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, sentiments, test_size=0.2, random_state=SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:51.298682Z",
     "start_time": "2024-03-27T22:49:51.141716Z"
    }
   },
   "id": "1798ae5c06e9b573",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pkl the train and test data\n",
    "train_df = pd.DataFrame({'text': X_train, 'sentiment': y_train})\n",
    "test_df = pd.DataFrame({'text': X_test, 'sentiment': y_test})\n",
    "train_df.to_pickle('beauty-train.pkl')\n",
    "test_df.to_pickle('beauty-test.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:51.584420Z",
     "start_time": "2024-03-27T22:49:51.300185Z"
    }
   },
   "id": "f158be4817dcb1e",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(434663, 108666)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:51.587467Z",
     "start_time": "2024-03-27T22:49:51.585204Z"
    }
   },
   "id": "9409a62bb82d449f",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "def generate_embeddings(texts):\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    batch_size = 256  # Adjust based on your system's memory capacity\n",
    "    embeddings = []\n",
    "    print('transforming data...')\n",
    "    num_batches = len(texts) // batch_size\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        if i % 100 == 0:\n",
    "            print(f'Processing batch {i // batch_size + 1}/{num_batches}')\n",
    "        batch = texts[i:i + batch_size]\n",
    "        batch_embeddings = embedding_model.encode(batch)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:49:52.880807Z",
     "start_time": "2024-03-27T22:49:52.878279Z"
    }
   },
   "id": "29533a78deb5e956",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming data...\n",
      "Processing batch 1/1697\n",
      "Processing batch 26/1697\n",
      "Processing batch 51/1697\n",
      "Processing batch 76/1697\n",
      "Processing batch 101/1697\n",
      "Processing batch 126/1697\n",
      "Processing batch 151/1697\n",
      "Processing batch 176/1697\n",
      "Processing batch 201/1697\n",
      "Processing batch 226/1697\n",
      "Processing batch 251/1697\n",
      "Processing batch 276/1697\n",
      "Processing batch 301/1697\n",
      "Processing batch 326/1697\n",
      "Processing batch 351/1697\n",
      "Processing batch 376/1697\n",
      "Processing batch 401/1697\n",
      "Processing batch 426/1697\n",
      "Processing batch 451/1697\n",
      "Processing batch 476/1697\n",
      "Processing batch 501/1697\n",
      "Processing batch 526/1697\n",
      "Processing batch 551/1697\n",
      "Processing batch 576/1697\n",
      "Processing batch 601/1697\n",
      "Processing batch 626/1697\n",
      "Processing batch 651/1697\n",
      "Processing batch 676/1697\n",
      "Processing batch 701/1697\n",
      "Processing batch 726/1697\n",
      "Processing batch 751/1697\n",
      "Processing batch 776/1697\n",
      "Processing batch 801/1697\n",
      "Processing batch 826/1697\n",
      "Processing batch 851/1697\n",
      "Processing batch 876/1697\n",
      "Processing batch 901/1697\n",
      "Processing batch 926/1697\n",
      "Processing batch 951/1697\n",
      "Processing batch 976/1697\n",
      "Processing batch 1001/1697\n",
      "Processing batch 1026/1697\n",
      "Processing batch 1051/1697\n",
      "Processing batch 1076/1697\n",
      "Processing batch 1101/1697\n",
      "Processing batch 1126/1697\n",
      "Processing batch 1151/1697\n",
      "Processing batch 1176/1697\n",
      "Processing batch 1201/1697\n",
      "Processing batch 1226/1697\n",
      "Processing batch 1251/1697\n",
      "Processing batch 1276/1697\n",
      "Processing batch 1301/1697\n",
      "Processing batch 1326/1697\n",
      "Processing batch 1351/1697\n",
      "Processing batch 1376/1697\n",
      "Processing batch 1401/1697\n",
      "Processing batch 1426/1697\n",
      "Processing batch 1451/1697\n",
      "Processing batch 1476/1697\n",
      "Processing batch 1501/1697\n",
      "Processing batch 1526/1697\n",
      "Processing batch 1551/1697\n",
      "Processing batch 1576/1697\n",
      "Processing batch 1601/1697\n",
      "Processing batch 1626/1697\n",
      "Processing batch 1651/1697\n",
      "Processing batch 1676/1697\n",
      "transforming data...\n",
      "Processing batch 1/424\n",
      "Processing batch 26/424\n",
      "Processing batch 51/424\n",
      "Processing batch 76/424\n",
      "Processing batch 101/424\n",
      "Processing batch 126/424\n",
      "Processing batch 151/424\n",
      "Processing batch 176/424\n",
      "Processing batch 201/424\n",
      "Processing batch 226/424\n",
      "Processing batch 251/424\n",
      "Processing batch 276/424\n",
      "Processing batch 301/424\n",
      "Processing batch 326/424\n",
      "Processing batch 351/424\n",
      "Processing batch 376/424\n",
      "Processing batch 401/424\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = generate_embeddings(X_train)\n",
    "test_embeddings = generate_embeddings(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:54:44.463288Z",
     "start_time": "2024-03-27T22:49:53.696837Z"
    }
   },
   "id": "e4295cf57fe059d",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_embeddings = np.array(train_embeddings)\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "\n",
    "# save out as pkl\n",
    "with open('beauty-train-embeddings.pkl', 'wb') as file:\n",
    "    pkl.dump(train_embeddings, file)\n",
    "    \n",
    "with open('beauty-test-embeddings.pkl', 'wb') as file:\n",
    "    pkl.dump(test_embeddings, file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:54:46.891167Z",
     "start_time": "2024-03-27T22:54:44.464912Z"
    }
   },
   "id": "ada04c5f290eef0b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "((434663, 384), (108666, 384))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape, test_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:54:46.894957Z",
     "start_time": "2024-03-27T22:54:46.892234Z"
    }
   },
   "id": "a7df673e17b649d0",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8616767472731749\n",
      "Test Accuracy: 0.861879520733256\n"
     ]
    }
   ],
   "source": [
    "# classifying the embeddings as a sanity check\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(train_embeddings, y_train)\n",
    "\n",
    "train_predictions = classifier.predict(train_embeddings)\n",
    "test_predictions = classifier.predict(test_embeddings)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print('Train Accuracy:', train_accuracy)\n",
    "print('Test Accuracy:', test_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T22:54:54.019289Z",
     "start_time": "2024-03-27T22:54:46.896310Z"
    }
   },
   "id": "c5ee64cc81dbaecd",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 sentences. Showing the first 10:\n",
      "Sample 0: This is an example sentence to demonstrate how to randomly remove words.\n",
      "Sample 1: is an sentence to demonstrate to remove\n",
      "Sample 2: This demonstrate\n",
      "Sample 3: This demonstrate remove words.\n",
      "Sample 4: demonstrate how remove words.\n",
      "Sample 5: This is an example to demonstrate how to randomly remove words.\n",
      "Sample 6: This an example sentence demonstrate how randomly remove words.\n",
      "Sample 7: example sentence demonstrate how randomly words.\n",
      "Sample 8: is an to demonstrate how to randomly remove words.\n",
      "Sample 9: is to to remove\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_ordered_sentence_neighborhood(sentence, num_samples=5000):\n",
    "    words = sentence.split()\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize the list to store neighborhood sentences\n",
    "    neighborhood_sentences = [sentence]  # Include the original sentence as the first sample\n",
    "    \n",
    "    for _ in range(num_samples - 1):  # We already have the original sentence, hence num_samples - 1\n",
    "        num_words_to_remove = np.random.randint(1, num_words)  # Number of words to remove\n",
    "        words_to_remove = np.random.choice(range(num_words), size=num_words_to_remove, replace=False)\n",
    "        perturbed_sentence = ' '.join([word for idx, word in enumerate(words) if idx not in words_to_remove])\n",
    "        neighborhood_sentences.append(perturbed_sentence)\n",
    "    \n",
    "    return neighborhood_sentences\n",
    "\n",
    "# Example usage:\n",
    "original_sentence = \"This is an example sentence to demonstrate how to randomly remove words.\"\n",
    "\n",
    "# Generating 5000 sentences (including the original)\n",
    "neighborhood = generate_ordered_sentence_neighborhood(original_sentence)\n",
    "print(f\"Generated {len(neighborhood)} sentences. Showing the first 10:\")\n",
    "for idx, sentence in enumerate(neighborhood[:10]):\n",
    "    print(f\"Sample {idx}: {sentence}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T17:51:21.209033Z",
     "start_time": "2024-03-28T17:51:21.062324Z"
    }
   },
   "id": "1579420c4ce4d5b",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39be94e14c1cdd5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llm_explainer",
   "language": "python",
   "display_name": "LLM_Explainer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
